Layer base_model.model.pos_embed sparsity: 0.28%
Layer base_model.model.pos_embed_token sparsity: 1.11%
Layer base_model.model.patch_embed.proj.weight sparsity: 0.29%
Layer base_model.model.patch_embed.proj.bias sparsity: 0.78%
Layer base_model.model.patch_embed.proj.lora_A.default.weight sparsity: 0.00%
Layer base_model.model.patch_embed.proj.lora_B.default.weight sparsity: 52.55%   *patch_embed
Layer base_model.model.text_embed.weight sparsity: 0.49%
Layer base_model.model.text_embed.bias sparsity: 0.52%
Layer base_model.model.text_embed.lora_A.default.weight sparsity: 0.12%
Layer base_model.model.text_embed.lora_B.default.weight sparsity: 18.57%
Layer base_model.model.text_out.weight sparsity: 4.68%
Layer base_model.model.text_out.bias sparsity: 10.94%
Layer base_model.model.clip_img_embed.weight sparsity: 0.32%
Layer base_model.model.clip_img_embed.bias sparsity: 0.00%
Layer base_model.model.clip_img_embed.lora_A.default.weight sparsity: 0.24%
Layer base_model.model.clip_img_embed.lora_B.default.weight sparsity: 15.58%
Layer base_model.model.clip_img_out.weight sparsity: 3.63%
Layer base_model.model.clip_img_out.bias sparsity: 4.30%
Layer base_model.model.in_blocks.0.norm2.weight sparsity: 0.13%
Layer base_model.model.in_blocks.0.norm2.bias sparsity: 0.46%
Layer base_model.model.in_blocks.0.attn.qkv.weight sparsity: 0.75%
Layer base_model.model.in_blocks.0.attn.qkv.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.0.attn.qkv.lora_B.default.weight sparsity: 19.15%
Layer base_model.model.in_blocks.0.attn.proj.weight sparsity: 1.20%
Layer base_model.model.in_blocks.0.attn.proj.bias sparsity: 0.00%
Layer base_model.model.in_blocks.0.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.0.attn.proj.lora_B.default.weight sparsity: 20.90%
Layer base_model.model.in_blocks.0.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.0.norm3.bias sparsity: 0.52%
Layer base_model.model.in_blocks.0.mlp.fc1.weight sparsity: 0.28%
Layer base_model.model.in_blocks.0.mlp.fc1.bias sparsity: 0.52%
Layer base_model.model.in_blocks.0.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.0.mlp.fc1.lora_B.default.weight sparsity: 16.72%
Layer base_model.model.in_blocks.0.mlp.fc2.weight sparsity: 0.31%
Layer base_model.model.in_blocks.0.mlp.fc2.bias sparsity: 0.46%
Layer base_model.model.in_blocks.0.mlp.fc2.lora_A.default.weight sparsity: 0.77%
Layer base_model.model.in_blocks.0.mlp.fc2.lora_B.default.weight sparsity: 16.84%
Layer base_model.model.in_blocks.1.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.1.norm2.bias sparsity: 0.13%
Layer base_model.model.in_blocks.1.attn.qkv.weight sparsity: 0.28%
Layer base_model.model.in_blocks.1.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.1.attn.qkv.lora_B.default.weight sparsity: 16.78%
Layer base_model.model.in_blocks.1.attn.proj.weight sparsity: 0.43%
Layer base_model.model.in_blocks.1.attn.proj.bias sparsity: 0.78%
Layer base_model.model.in_blocks.1.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.1.attn.proj.lora_B.default.weight sparsity: 16.97%
Layer base_model.model.in_blocks.1.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.1.norm3.bias sparsity: 0.85%
Layer base_model.model.in_blocks.1.mlp.fc1.weight sparsity: 0.28%
Layer base_model.model.in_blocks.1.mlp.fc1.bias sparsity: 0.08%
Layer base_model.model.in_blocks.1.mlp.fc1.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.1.mlp.fc1.lora_B.default.weight sparsity: 19.21%
Layer base_model.model.in_blocks.1.mlp.fc2.weight sparsity: 0.39%
Layer base_model.model.in_blocks.1.mlp.fc2.bias sparsity: 0.52%
Layer base_model.model.in_blocks.1.mlp.fc2.lora_A.default.weight sparsity: 0.80%
Layer base_model.model.in_blocks.1.mlp.fc2.lora_B.default.weight sparsity: 15.74%
Layer base_model.model.in_blocks.2.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.2.norm2.bias sparsity: 0.07%
Layer base_model.model.in_blocks.2.attn.qkv.weight sparsity: 0.23%
Layer base_model.model.in_blocks.2.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.2.attn.qkv.lora_B.default.weight sparsity: 17.29%
Layer base_model.model.in_blocks.2.attn.proj.weight sparsity: 0.28%
Layer base_model.model.in_blocks.2.attn.proj.bias sparsity: 0.72%
Layer base_model.model.in_blocks.2.attn.proj.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.in_blocks.2.attn.proj.lora_B.default.weight sparsity: 16.18%
Layer base_model.model.in_blocks.2.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.2.norm3.bias sparsity: 1.04%
Layer base_model.model.in_blocks.2.mlp.fc1.weight sparsity: 0.26%
Layer base_model.model.in_blocks.2.mlp.fc1.bias sparsity: 0.11%
Layer base_model.model.in_blocks.2.mlp.fc1.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.in_blocks.2.mlp.fc1.lora_B.default.weight sparsity: 17.53%
Layer base_model.model.in_blocks.2.mlp.fc2.weight sparsity: 0.39%
Layer base_model.model.in_blocks.2.mlp.fc2.bias sparsity: 0.46%
Layer base_model.model.in_blocks.2.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.in_blocks.2.mlp.fc2.lora_B.default.weight sparsity: 15.47%
Layer base_model.model.in_blocks.3.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.3.norm2.bias sparsity: 0.07%
Layer base_model.model.in_blocks.3.attn.qkv.weight sparsity: 0.22%
Layer base_model.model.in_blocks.3.attn.qkv.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.3.attn.qkv.lora_B.default.weight sparsity: 18.33%
Layer base_model.model.in_blocks.3.attn.proj.weight sparsity: 0.29%
Layer base_model.model.in_blocks.3.attn.proj.bias sparsity: 0.59%
Layer base_model.model.in_blocks.3.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.3.attn.proj.lora_B.default.weight sparsity: 16.13%
Layer base_model.model.in_blocks.3.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.3.norm3.bias sparsity: 0.65%
Layer base_model.model.in_blocks.3.mlp.fc1.weight sparsity: 0.21%
Layer base_model.model.in_blocks.3.mlp.fc1.bias sparsity: 0.29%
Layer base_model.model.in_blocks.3.mlp.fc1.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.3.mlp.fc1.lora_B.default.weight sparsity: 16.85%
Layer base_model.model.in_blocks.3.mlp.fc2.weight sparsity: 0.25%
Layer base_model.model.in_blocks.3.mlp.fc2.bias sparsity: 0.52%
Layer base_model.model.in_blocks.3.mlp.fc2.lora_A.default.weight sparsity: 0.80%
Layer base_model.model.in_blocks.3.mlp.fc2.lora_B.default.weight sparsity: 15.26%
Layer base_model.model.in_blocks.4.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.4.norm2.bias sparsity: 0.00%
Layer base_model.model.in_blocks.4.attn.qkv.weight sparsity: 0.21%
Layer base_model.model.in_blocks.4.attn.qkv.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.4.attn.qkv.lora_B.default.weight sparsity: 17.33%
Layer base_model.model.in_blocks.4.attn.proj.weight sparsity: 0.26%
Layer base_model.model.in_blocks.4.attn.proj.bias sparsity: 0.59%
Layer base_model.model.in_blocks.4.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.4.attn.proj.lora_B.default.weight sparsity: 16.70%
Layer base_model.model.in_blocks.4.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.4.norm3.bias sparsity: 0.39%
Layer base_model.model.in_blocks.4.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.4.mlp.fc1.bias sparsity: 0.23%
Layer base_model.model.in_blocks.4.mlp.fc1.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.4.mlp.fc1.lora_B.default.weight sparsity: 16.67%
Layer base_model.model.in_blocks.4.mlp.fc2.weight sparsity: 0.23%
Layer base_model.model.in_blocks.4.mlp.fc2.bias sparsity: 0.72%
Layer base_model.model.in_blocks.4.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.in_blocks.4.mlp.fc2.lora_B.default.weight sparsity: 15.16%
Layer base_model.model.in_blocks.5.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.5.norm2.bias sparsity: 0.07%
Layer base_model.model.in_blocks.5.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.in_blocks.5.attn.qkv.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.5.attn.qkv.lora_B.default.weight sparsity: 17.00%
Layer base_model.model.in_blocks.5.attn.proj.weight sparsity: 0.23%
Layer base_model.model.in_blocks.5.attn.proj.bias sparsity: 1.30%
Layer base_model.model.in_blocks.5.attn.proj.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.in_blocks.5.attn.proj.lora_B.default.weight sparsity: 17.22%
Layer base_model.model.in_blocks.5.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.5.norm3.bias sparsity: 0.13%
Layer base_model.model.in_blocks.5.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.5.mlp.fc1.bias sparsity: 0.13%
Layer base_model.model.in_blocks.5.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.5.mlp.fc1.lora_B.default.weight sparsity: 16.74%
Layer base_model.model.in_blocks.5.mlp.fc2.weight sparsity: 0.23%
Layer base_model.model.in_blocks.5.mlp.fc2.bias sparsity: 0.26%
Layer base_model.model.in_blocks.5.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.in_blocks.5.mlp.fc2.lora_B.default.weight sparsity: 15.42%
Layer base_model.model.in_blocks.6.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.6.norm2.bias sparsity: 0.13%
Layer base_model.model.in_blocks.6.attn.qkv.weight sparsity: 0.17%
Layer base_model.model.in_blocks.6.attn.qkv.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.in_blocks.6.attn.qkv.lora_B.default.weight sparsity: 17.97%
Layer base_model.model.in_blocks.6.attn.proj.weight sparsity: 0.23%
Layer base_model.model.in_blocks.6.attn.proj.bias sparsity: 0.65%
Layer base_model.model.in_blocks.6.attn.proj.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.in_blocks.6.attn.proj.lora_B.default.weight sparsity: 18.22%
Layer base_model.model.in_blocks.6.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.6.norm3.bias sparsity: 0.26%
Layer base_model.model.in_blocks.6.mlp.fc1.weight sparsity: 0.19%
Layer base_model.model.in_blocks.6.mlp.fc1.bias sparsity: 0.10%
Layer base_model.model.in_blocks.6.mlp.fc1.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.6.mlp.fc1.lora_B.default.weight sparsity: 17.28%
Layer base_model.model.in_blocks.6.mlp.fc2.weight sparsity: 0.22%
Layer base_model.model.in_blocks.6.mlp.fc2.bias sparsity: 0.46%
Layer base_model.model.in_blocks.6.mlp.fc2.lora_A.default.weight sparsity: 0.81%
Layer base_model.model.in_blocks.6.mlp.fc2.lora_B.default.weight sparsity: 15.52%
Layer base_model.model.in_blocks.7.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.7.norm2.bias sparsity: 0.13%
Layer base_model.model.in_blocks.7.attn.qkv.weight sparsity: 0.17%
Layer base_model.model.in_blocks.7.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.7.attn.qkv.lora_B.default.weight sparsity: 17.22%
Layer base_model.model.in_blocks.7.attn.proj.weight sparsity: 0.25%
Layer base_model.model.in_blocks.7.attn.proj.bias sparsity: 0.59%
Layer base_model.model.in_blocks.7.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.7.attn.proj.lora_B.default.weight sparsity: 16.70%
Layer base_model.model.in_blocks.7.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.7.norm3.bias sparsity: 0.07%
Layer base_model.model.in_blocks.7.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.7.mlp.fc1.bias sparsity: 0.28%
Layer base_model.model.in_blocks.7.mlp.fc1.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.7.mlp.fc1.lora_B.default.weight sparsity: 16.38%
Layer base_model.model.in_blocks.7.mlp.fc2.weight sparsity: 0.24%
Layer base_model.model.in_blocks.7.mlp.fc2.bias sparsity: 0.46%
Layer base_model.model.in_blocks.7.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.in_blocks.7.mlp.fc2.lora_B.default.weight sparsity: 15.61%
Layer base_model.model.in_blocks.8.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.8.norm2.bias sparsity: 0.07%
Layer base_model.model.in_blocks.8.attn.qkv.weight sparsity: 0.18%
Layer base_model.model.in_blocks.8.attn.qkv.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.8.attn.qkv.lora_B.default.weight sparsity: 16.27%
Layer base_model.model.in_blocks.8.attn.proj.weight sparsity: 0.25%
Layer base_model.model.in_blocks.8.attn.proj.bias sparsity: 0.91%
Layer base_model.model.in_blocks.8.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.8.attn.proj.lora_B.default.weight sparsity: 15.89%
Layer base_model.model.in_blocks.8.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.8.norm3.bias sparsity: 0.13%
Layer base_model.model.in_blocks.8.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.8.mlp.fc1.bias sparsity: 0.23%
Layer base_model.model.in_blocks.8.mlp.fc1.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.8.mlp.fc1.lora_B.default.weight sparsity: 15.78%
Layer base_model.model.in_blocks.8.mlp.fc2.weight sparsity: 0.24%
Layer base_model.model.in_blocks.8.mlp.fc2.bias sparsity: 0.39%
Layer base_model.model.in_blocks.8.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.in_blocks.8.mlp.fc2.lora_B.default.weight sparsity: 14.84%
Layer base_model.model.in_blocks.9.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.9.norm2.bias sparsity: 0.13%
Layer base_model.model.in_blocks.9.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.in_blocks.9.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.9.attn.qkv.lora_B.default.weight sparsity: 15.59%
Layer base_model.model.in_blocks.9.attn.proj.weight sparsity: 0.27%
Layer base_model.model.in_blocks.9.attn.proj.bias sparsity: 0.72%
Layer base_model.model.in_blocks.9.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.9.attn.proj.lora_B.default.weight sparsity: 15.63%
Layer base_model.model.in_blocks.9.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.9.norm3.bias sparsity: 0.39%
Layer base_model.model.in_blocks.9.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.9.mlp.fc1.bias sparsity: 0.31%
Layer base_model.model.in_blocks.9.mlp.fc1.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.9.mlp.fc1.lora_B.default.weight sparsity: 15.66%
Layer base_model.model.in_blocks.9.mlp.fc2.weight sparsity: 0.23%
Layer base_model.model.in_blocks.9.mlp.fc2.bias sparsity: 0.33%
Layer base_model.model.in_blocks.9.mlp.fc2.lora_A.default.weight sparsity: 0.77%
Layer base_model.model.in_blocks.9.mlp.fc2.lora_B.default.weight sparsity: 13.70%
Layer base_model.model.in_blocks.10.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.10.norm2.bias sparsity: 0.13%
Layer base_model.model.in_blocks.10.attn.qkv.weight sparsity: 0.18%
Layer base_model.model.in_blocks.10.attn.qkv.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.in_blocks.10.attn.qkv.lora_B.default.weight sparsity: 15.24%
Layer base_model.model.in_blocks.10.attn.proj.weight sparsity: 0.26%
Layer base_model.model.in_blocks.10.attn.proj.bias sparsity: 0.39%
Layer base_model.model.in_blocks.10.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.10.attn.proj.lora_B.default.weight sparsity: 14.64%
Layer base_model.model.in_blocks.10.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.10.norm3.bias sparsity: 0.52%
Layer base_model.model.in_blocks.10.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.10.mlp.fc1.bias sparsity: 0.28%
Layer base_model.model.in_blocks.10.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.10.mlp.fc1.lora_B.default.weight sparsity: 14.95%
Layer base_model.model.in_blocks.10.mlp.fc2.weight sparsity: 0.23%
Layer base_model.model.in_blocks.10.mlp.fc2.bias sparsity: 0.39%
Layer base_model.model.in_blocks.10.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.in_blocks.10.mlp.fc2.lora_B.default.weight sparsity: 14.99%
Layer base_model.model.in_blocks.11.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.11.norm2.bias sparsity: 0.20%
Layer base_model.model.in_blocks.11.attn.qkv.weight sparsity: 0.18%
Layer base_model.model.in_blocks.11.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.11.attn.qkv.lora_B.default.weight sparsity: 15.66%
Layer base_model.model.in_blocks.11.attn.proj.weight sparsity: 0.25%
Layer base_model.model.in_blocks.11.attn.proj.bias sparsity: 0.52%
Layer base_model.model.in_blocks.11.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.11.attn.proj.lora_B.default.weight sparsity: 14.80%
Layer base_model.model.in_blocks.11.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.11.norm3.bias sparsity: 0.39%
Layer base_model.model.in_blocks.11.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.11.mlp.fc1.bias sparsity: 0.37%
Layer base_model.model.in_blocks.11.mlp.fc1.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.11.mlp.fc1.lora_B.default.weight sparsity: 15.03%
Layer base_model.model.in_blocks.11.mlp.fc2.weight sparsity: 0.23%
Layer base_model.model.in_blocks.11.mlp.fc2.bias sparsity: 0.98%
Layer base_model.model.in_blocks.11.mlp.fc2.lora_A.default.weight sparsity: 0.77%
Layer base_model.model.in_blocks.11.mlp.fc2.lora_B.default.weight sparsity: 14.89%
Layer base_model.model.in_blocks.12.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.12.norm2.bias sparsity: 0.00%
Layer base_model.model.in_blocks.12.attn.qkv.weight sparsity: 0.18%
Layer base_model.model.in_blocks.12.attn.qkv.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.12.attn.qkv.lora_B.default.weight sparsity: 15.22%
Layer base_model.model.in_blocks.12.attn.proj.weight sparsity: 0.23%
Layer base_model.model.in_blocks.12.attn.proj.bias sparsity: 0.13%
Layer base_model.model.in_blocks.12.attn.proj.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.12.attn.proj.lora_B.default.weight sparsity: 14.09%
Layer base_model.model.in_blocks.12.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.12.norm3.bias sparsity: 0.07%
Layer base_model.model.in_blocks.12.mlp.fc1.weight sparsity: 0.19%
Layer base_model.model.in_blocks.12.mlp.fc1.bias sparsity: 0.52%
Layer base_model.model.in_blocks.12.mlp.fc1.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.12.mlp.fc1.lora_B.default.weight sparsity: 13.65%
Layer base_model.model.in_blocks.12.mlp.fc2.weight sparsity: 0.21%
Layer base_model.model.in_blocks.12.mlp.fc2.bias sparsity: 0.78%
Layer base_model.model.in_blocks.12.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.in_blocks.12.mlp.fc2.lora_B.default.weight sparsity: 12.47%
Layer base_model.model.in_blocks.13.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.13.norm2.bias sparsity: 0.07%
Layer base_model.model.in_blocks.13.attn.qkv.weight sparsity: 0.18%
Layer base_model.model.in_blocks.13.attn.qkv.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.13.attn.qkv.lora_B.default.weight sparsity: 15.34%
Layer base_model.model.in_blocks.13.attn.proj.weight sparsity: 0.24%
Layer base_model.model.in_blocks.13.attn.proj.bias sparsity: 0.26%
Layer base_model.model.in_blocks.13.attn.proj.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.13.attn.proj.lora_B.default.weight sparsity: 13.29%
Layer base_model.model.in_blocks.13.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.13.norm3.bias sparsity: 0.33%
Layer base_model.model.in_blocks.13.mlp.fc1.weight sparsity: 0.19%
Layer base_model.model.in_blocks.13.mlp.fc1.bias sparsity: 0.20%
Layer base_model.model.in_blocks.13.mlp.fc1.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.13.mlp.fc1.lora_B.default.weight sparsity: 12.92%
Layer base_model.model.in_blocks.13.mlp.fc2.weight sparsity: 0.22%
Layer base_model.model.in_blocks.13.mlp.fc2.bias sparsity: 0.39%
Layer base_model.model.in_blocks.13.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.in_blocks.13.mlp.fc2.lora_B.default.weight sparsity: 11.57%
Layer base_model.model.in_blocks.14.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.14.norm2.bias sparsity: 0.07%
Layer base_model.model.in_blocks.14.attn.qkv.weight sparsity: 0.20%
Layer base_model.model.in_blocks.14.attn.qkv.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.14.attn.qkv.lora_B.default.weight sparsity: 14.76%
Layer base_model.model.in_blocks.14.attn.proj.weight sparsity: 0.25%
Layer base_model.model.in_blocks.14.attn.proj.bias sparsity: 0.72%
Layer base_model.model.in_blocks.14.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.14.attn.proj.lora_B.default.weight sparsity: 13.33%
Layer base_model.model.in_blocks.14.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.14.norm3.bias sparsity: 0.07%
Layer base_model.model.in_blocks.14.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.14.mlp.fc1.bias sparsity: 0.10%
Layer base_model.model.in_blocks.14.mlp.fc1.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.in_blocks.14.mlp.fc1.lora_B.default.weight sparsity: 13.78%
Layer base_model.model.in_blocks.14.mlp.fc2.weight sparsity: 0.23%
Layer base_model.model.in_blocks.14.mlp.fc2.bias sparsity: 0.52%
Layer base_model.model.in_blocks.14.mlp.fc2.lora_A.default.weight sparsity: 0.77%
Layer base_model.model.in_blocks.14.mlp.fc2.lora_B.default.weight sparsity: 13.11%
Layer base_model.model.mid_block.norm2.weight sparsity: 0.00%
Layer base_model.model.mid_block.norm2.bias sparsity: 0.00%
Layer base_model.model.mid_block.attn.qkv.weight sparsity: 0.23%
Layer base_model.model.mid_block.attn.qkv.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.mid_block.attn.qkv.lora_B.default.weight sparsity: 15.69%
Layer base_model.model.mid_block.attn.proj.weight sparsity: 0.27%
Layer base_model.model.mid_block.attn.proj.bias sparsity: 0.20%
Layer base_model.model.mid_block.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.mid_block.attn.proj.lora_B.default.weight sparsity: 14.41%
Layer base_model.model.mid_block.norm3.weight sparsity: 0.00%
Layer base_model.model.mid_block.norm3.bias sparsity: 0.33%
Layer base_model.model.mid_block.mlp.fc1.weight sparsity: 0.22%
Layer base_model.model.mid_block.mlp.fc1.bias sparsity: 0.34%
Layer base_model.model.mid_block.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.mid_block.mlp.fc1.lora_B.default.weight sparsity: 13.73%
Layer base_model.model.mid_block.mlp.fc2.weight sparsity: 0.26%
Layer base_model.model.mid_block.mlp.fc2.bias sparsity: 0.72%
Layer base_model.model.mid_block.mlp.fc2.lora_A.default.weight sparsity: 0.77%
Layer base_model.model.mid_block.mlp.fc2.lora_B.default.weight sparsity: 13.43%
Layer base_model.model.out_blocks.0.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.0.norm1.bias sparsity: 0.20%
Layer base_model.model.out_blocks.0.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.0.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.0.attn.qkv.weight sparsity: 0.28%
Layer base_model.model.out_blocks.0.attn.qkv.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.out_blocks.0.attn.qkv.lora_B.default.weight sparsity: 14.72%
Layer base_model.model.out_blocks.0.attn.proj.weight sparsity: 0.29%
Layer base_model.model.out_blocks.0.attn.proj.bias sparsity: 0.20%
Layer base_model.model.out_blocks.0.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.0.attn.proj.lora_B.default.weight sparsity: 12.74%
Layer base_model.model.out_blocks.0.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.0.norm3.bias sparsity: 0.91%
Layer base_model.model.out_blocks.0.mlp.fc1.weight sparsity: 0.24%
Layer base_model.model.out_blocks.0.mlp.fc1.bias sparsity: 0.33%
Layer base_model.model.out_blocks.0.mlp.fc1.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.0.mlp.fc1.lora_B.default.weight sparsity: 12.42%
Layer base_model.model.out_blocks.0.mlp.fc2.weight sparsity: 0.31%
Layer base_model.model.out_blocks.0.mlp.fc2.bias sparsity: 0.91%
Layer base_model.model.out_blocks.0.mlp.fc2.lora_A.default.weight sparsity: 0.77%
Layer base_model.model.out_blocks.0.mlp.fc2.lora_B.default.weight sparsity: 11.98%
Layer base_model.model.out_blocks.0.skip_linear.weight sparsity: 0.27%
Layer base_model.model.out_blocks.0.skip_linear.bias sparsity: 0.13%
Layer base_model.model.out_blocks.1.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.1.norm1.bias sparsity: 0.33%
Layer base_model.model.out_blocks.1.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.1.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.1.attn.qkv.weight sparsity: 0.21%
Layer base_model.model.out_blocks.1.attn.qkv.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.out_blocks.1.attn.qkv.lora_B.default.weight sparsity: 12.22%
Layer base_model.model.out_blocks.1.attn.proj.weight sparsity: 0.24%
Layer base_model.model.out_blocks.1.attn.proj.bias sparsity: 0.13%
Layer base_model.model.out_blocks.1.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.1.attn.proj.lora_B.default.weight sparsity: 11.20%
Layer base_model.model.out_blocks.1.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.1.norm3.bias sparsity: 0.39%
Layer base_model.model.out_blocks.1.mlp.fc1.weight sparsity: 0.19%
Layer base_model.model.out_blocks.1.mlp.fc1.bias sparsity: 0.31%
Layer base_model.model.out_blocks.1.mlp.fc1.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.1.mlp.fc1.lora_B.default.weight sparsity: 10.62%
Layer base_model.model.out_blocks.1.mlp.fc2.weight sparsity: 0.22%
Layer base_model.model.out_blocks.1.mlp.fc2.bias sparsity: 0.65%
Layer base_model.model.out_blocks.1.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.out_blocks.1.mlp.fc2.lora_B.default.weight sparsity: 12.41%
Layer base_model.model.out_blocks.1.skip_linear.weight sparsity: 0.22%
Layer base_model.model.out_blocks.1.skip_linear.bias sparsity: 1.11%
Layer base_model.model.out_blocks.2.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.2.norm1.bias sparsity: 0.65%
Layer base_model.model.out_blocks.2.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.2.norm2.bias sparsity: 0.00%
Layer base_model.model.out_blocks.2.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.2.attn.qkv.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.2.attn.qkv.lora_B.default.weight sparsity: 12.00%
Layer base_model.model.out_blocks.2.attn.proj.weight sparsity: 0.24%
Layer base_model.model.out_blocks.2.attn.proj.bias sparsity: 0.07%
Layer base_model.model.out_blocks.2.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.2.attn.proj.lora_B.default.weight sparsity: 11.21%
Layer base_model.model.out_blocks.2.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.2.norm3.bias sparsity: 0.33%
Layer base_model.model.out_blocks.2.mlp.fc1.weight sparsity: 0.19%
Layer base_model.model.out_blocks.2.mlp.fc1.bias sparsity: 0.31%
Layer base_model.model.out_blocks.2.mlp.fc1.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.out_blocks.2.mlp.fc1.lora_B.default.weight sparsity: 10.55%
Layer base_model.model.out_blocks.2.mlp.fc2.weight sparsity: 0.22%
Layer base_model.model.out_blocks.2.mlp.fc2.bias sparsity: 1.04%
Layer base_model.model.out_blocks.2.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.out_blocks.2.mlp.fc2.lora_B.default.weight sparsity: 12.98%
Layer base_model.model.out_blocks.2.skip_linear.weight sparsity: 0.20%
Layer base_model.model.out_blocks.2.skip_linear.bias sparsity: 0.91%
Layer base_model.model.out_blocks.3.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.3.norm1.bias sparsity: 0.07%
Layer base_model.model.out_blocks.3.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.3.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.3.attn.qkv.weight sparsity: 0.20%
Layer base_model.model.out_blocks.3.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.3.attn.qkv.lora_B.default.weight sparsity: 13.40%
Layer base_model.model.out_blocks.3.attn.proj.weight sparsity: 0.28%
Layer base_model.model.out_blocks.3.attn.proj.bias sparsity: 0.59%
Layer base_model.model.out_blocks.3.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.3.attn.proj.lora_B.default.weight sparsity: 11.86%
Layer base_model.model.out_blocks.3.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.3.norm3.bias sparsity: 0.26%
Layer base_model.model.out_blocks.3.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.out_blocks.3.mlp.fc1.bias sparsity: 0.42%
Layer base_model.model.out_blocks.3.mlp.fc1.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.3.mlp.fc1.lora_B.default.weight sparsity: 13.32%
Layer base_model.model.out_blocks.3.mlp.fc2.weight sparsity: 0.24%
Layer base_model.model.out_blocks.3.mlp.fc2.bias sparsity: 0.33%
Layer base_model.model.out_blocks.3.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.out_blocks.3.mlp.fc2.lora_B.default.weight sparsity: 13.99%
Layer base_model.model.out_blocks.3.skip_linear.weight sparsity: 0.20%
Layer base_model.model.out_blocks.3.skip_linear.bias sparsity: 0.52%
Layer base_model.model.out_blocks.4.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.4.norm1.bias sparsity: 0.13%
Layer base_model.model.out_blocks.4.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.4.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.4.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.4.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.4.attn.qkv.lora_B.default.weight sparsity: 13.89%
Layer base_model.model.out_blocks.4.attn.proj.weight sparsity: 0.28%
Layer base_model.model.out_blocks.4.attn.proj.bias sparsity: 0.46%
Layer base_model.model.out_blocks.4.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.4.attn.proj.lora_B.default.weight sparsity: 12.31%
Layer base_model.model.out_blocks.4.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.4.norm3.bias sparsity: 0.26%
Layer base_model.model.out_blocks.4.mlp.fc1.weight sparsity: 0.21%
Layer base_model.model.out_blocks.4.mlp.fc1.bias sparsity: 0.34%
Layer base_model.model.out_blocks.4.mlp.fc1.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.out_blocks.4.mlp.fc1.lora_B.default.weight sparsity: 13.99%
Layer base_model.model.out_blocks.4.mlp.fc2.weight sparsity: 0.26%
Layer base_model.model.out_blocks.4.mlp.fc2.bias sparsity: 0.07%
Layer base_model.model.out_blocks.4.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.out_blocks.4.mlp.fc2.lora_B.default.weight sparsity: 15.57%
Layer base_model.model.out_blocks.4.skip_linear.weight sparsity: 0.22%
Layer base_model.model.out_blocks.4.skip_linear.bias sparsity: 0.65%
Layer base_model.model.out_blocks.5.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.5.norm1.bias sparsity: 0.26%
Layer base_model.model.out_blocks.5.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.5.norm2.bias sparsity: 0.20%
Layer base_model.model.out_blocks.5.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.5.attn.qkv.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.5.attn.qkv.lora_B.default.weight sparsity: 14.73%
Layer base_model.model.out_blocks.5.attn.proj.weight sparsity: 0.26%
Layer base_model.model.out_blocks.5.attn.proj.bias sparsity: 0.46%
Layer base_model.model.out_blocks.5.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.5.attn.proj.lora_B.default.weight sparsity: 13.18%
Layer base_model.model.out_blocks.5.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.5.norm3.bias sparsity: 0.20%
Layer base_model.model.out_blocks.5.mlp.fc1.weight sparsity: 0.21%
Layer base_model.model.out_blocks.5.mlp.fc1.bias sparsity: 0.39%
Layer base_model.model.out_blocks.5.mlp.fc1.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.5.mlp.fc1.lora_B.default.weight sparsity: 14.54%
Layer base_model.model.out_blocks.5.mlp.fc2.weight sparsity: 0.25%
Layer base_model.model.out_blocks.5.mlp.fc2.bias sparsity: 0.46%
Layer base_model.model.out_blocks.5.mlp.fc2.lora_A.default.weight sparsity: 0.76%
Layer base_model.model.out_blocks.5.mlp.fc2.lora_B.default.weight sparsity: 14.78%
Layer base_model.model.out_blocks.5.skip_linear.weight sparsity: 0.23%
Layer base_model.model.out_blocks.5.skip_linear.bias sparsity: 0.72%
Layer base_model.model.out_blocks.6.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.6.norm1.bias sparsity: 0.26%
Layer base_model.model.out_blocks.6.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.6.norm2.bias sparsity: 0.07%
Layer base_model.model.out_blocks.6.attn.qkv.weight sparsity: 0.20%
Layer base_model.model.out_blocks.6.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.6.attn.qkv.lora_B.default.weight sparsity: 14.74%
Layer base_model.model.out_blocks.6.attn.proj.weight sparsity: 0.28%
Layer base_model.model.out_blocks.6.attn.proj.bias sparsity: 0.07%
Layer base_model.model.out_blocks.6.attn.proj.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.6.attn.proj.lora_B.default.weight sparsity: 14.17%
Layer base_model.model.out_blocks.6.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.6.norm3.bias sparsity: 0.39%
Layer base_model.model.out_blocks.6.mlp.fc1.weight sparsity: 0.21%
Layer base_model.model.out_blocks.6.mlp.fc1.bias sparsity: 0.31%
Layer base_model.model.out_blocks.6.mlp.fc1.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.out_blocks.6.mlp.fc1.lora_B.default.weight sparsity: 15.54%
Layer base_model.model.out_blocks.6.mlp.fc2.weight sparsity: 0.27%
Layer base_model.model.out_blocks.6.mlp.fc2.bias sparsity: 0.52%
Layer base_model.model.out_blocks.6.mlp.fc2.lora_A.default.weight sparsity: 0.77%
Layer base_model.model.out_blocks.6.mlp.fc2.lora_B.default.weight sparsity: 16.67%
Layer base_model.model.out_blocks.6.skip_linear.weight sparsity: 0.23%
Layer base_model.model.out_blocks.6.skip_linear.bias sparsity: 0.26%
Layer base_model.model.out_blocks.7.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.7.norm1.bias sparsity: 0.07%
Layer base_model.model.out_blocks.7.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.7.norm2.bias sparsity: 0.07%
Layer base_model.model.out_blocks.7.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.7.attn.qkv.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.7.attn.qkv.lora_B.default.weight sparsity: 15.92%
Layer base_model.model.out_blocks.7.attn.proj.weight sparsity: 0.27%
Layer base_model.model.out_blocks.7.attn.proj.bias sparsity: 0.59%
Layer base_model.model.out_blocks.7.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.7.attn.proj.lora_B.default.weight sparsity: 15.44%
Layer base_model.model.out_blocks.7.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.7.norm3.bias sparsity: 0.13%
Layer base_model.model.out_blocks.7.mlp.fc1.weight sparsity: 0.21%
Layer base_model.model.out_blocks.7.mlp.fc1.bias sparsity: 0.39%
Layer base_model.model.out_blocks.7.mlp.fc1.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.7.mlp.fc1.lora_B.default.weight sparsity: 16.26%
Layer base_model.model.out_blocks.7.mlp.fc2.weight sparsity: 0.27%
Layer base_model.model.out_blocks.7.mlp.fc2.bias sparsity: 0.20%
Layer base_model.model.out_blocks.7.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.out_blocks.7.mlp.fc2.lora_B.default.weight sparsity: 17.63%
Layer base_model.model.out_blocks.7.skip_linear.weight sparsity: 0.25%
Layer base_model.model.out_blocks.7.skip_linear.bias sparsity: 0.78%
Layer base_model.model.out_blocks.8.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.8.norm1.bias sparsity: 0.13%
Layer base_model.model.out_blocks.8.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.8.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.8.attn.qkv.weight sparsity: 0.20%
Layer base_model.model.out_blocks.8.attn.qkv.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.8.attn.qkv.lora_B.default.weight sparsity: 17.49%
Layer base_model.model.out_blocks.8.attn.proj.weight sparsity: 0.31%
Layer base_model.model.out_blocks.8.attn.proj.bias sparsity: 0.72%
Layer base_model.model.out_blocks.8.attn.proj.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.8.attn.proj.lora_B.default.weight sparsity: 17.12%
Layer base_model.model.out_blocks.8.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.8.norm3.bias sparsity: 0.33%
Layer base_model.model.out_blocks.8.mlp.fc1.weight sparsity: 0.21%
Layer base_model.model.out_blocks.8.mlp.fc1.bias sparsity: 0.08%
Layer base_model.model.out_blocks.8.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.8.mlp.fc1.lora_B.default.weight sparsity: 16.99%
Layer base_model.model.out_blocks.8.mlp.fc2.weight sparsity: 0.28%
Layer base_model.model.out_blocks.8.mlp.fc2.bias sparsity: 0.65%
Layer base_model.model.out_blocks.8.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.out_blocks.8.mlp.fc2.lora_B.default.weight sparsity: 18.48%
Layer base_model.model.out_blocks.8.skip_linear.weight sparsity: 0.29%
Layer base_model.model.out_blocks.8.skip_linear.bias sparsity: 0.46%
Layer base_model.model.out_blocks.9.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.9.norm1.bias sparsity: 0.00%
Layer base_model.model.out_blocks.9.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.9.norm2.bias sparsity: 0.07%
Layer base_model.model.out_blocks.9.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.9.attn.qkv.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.9.attn.qkv.lora_B.default.weight sparsity: 17.12%
Layer base_model.model.out_blocks.9.attn.proj.weight sparsity: 0.29%
Layer base_model.model.out_blocks.9.attn.proj.bias sparsity: 0.59%
Layer base_model.model.out_blocks.9.attn.proj.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.9.attn.proj.lora_B.default.weight sparsity: 18.17%
Layer base_model.model.out_blocks.9.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.9.norm3.bias sparsity: 0.33%
Layer base_model.model.out_blocks.9.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.out_blocks.9.mlp.fc1.bias sparsity: 0.26%
Layer base_model.model.out_blocks.9.mlp.fc1.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.9.mlp.fc1.lora_B.default.weight sparsity: 16.08%
Layer base_model.model.out_blocks.9.mlp.fc2.weight sparsity: 0.26%
Layer base_model.model.out_blocks.9.mlp.fc2.bias sparsity: 0.65%
Layer base_model.model.out_blocks.9.mlp.fc2.lora_A.default.weight sparsity: 0.81%
Layer base_model.model.out_blocks.9.mlp.fc2.lora_B.default.weight sparsity: 16.80%
Layer base_model.model.out_blocks.9.skip_linear.weight sparsity: 0.32%
Layer base_model.model.out_blocks.9.skip_linear.bias sparsity: 0.26%
Layer base_model.model.out_blocks.10.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.10.norm1.bias sparsity: 0.07%
Layer base_model.model.out_blocks.10.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.10.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.10.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.10.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.10.attn.qkv.lora_B.default.weight sparsity: 15.14%
Layer base_model.model.out_blocks.10.attn.proj.weight sparsity: 0.28%
Layer base_model.model.out_blocks.10.attn.proj.bias sparsity: 0.46%
Layer base_model.model.out_blocks.10.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.10.attn.proj.lora_B.default.weight sparsity: 16.68%
Layer base_model.model.out_blocks.10.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.10.norm3.bias sparsity: 0.26%
Layer base_model.model.out_blocks.10.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.out_blocks.10.mlp.fc1.bias sparsity: 0.24%
Layer base_model.model.out_blocks.10.mlp.fc1.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.10.mlp.fc1.lora_B.default.weight sparsity: 13.35%
Layer base_model.model.out_blocks.10.mlp.fc2.weight sparsity: 0.24%
Layer base_model.model.out_blocks.10.mlp.fc2.bias sparsity: 0.98%
Layer base_model.model.out_blocks.10.mlp.fc2.lora_A.default.weight sparsity: 0.76%
Layer base_model.model.out_blocks.10.mlp.fc2.lora_B.default.weight sparsity: 14.31%
Layer base_model.model.out_blocks.10.skip_linear.weight sparsity: 0.25%
Layer base_model.model.out_blocks.10.skip_linear.bias sparsity: 0.39%
Layer base_model.model.out_blocks.11.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.11.norm1.bias sparsity: 0.20%
Layer base_model.model.out_blocks.11.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.11.norm2.bias sparsity: 0.07%
Layer base_model.model.out_blocks.11.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.11.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.11.attn.qkv.lora_B.default.weight sparsity: 12.29%
Layer base_model.model.out_blocks.11.attn.proj.weight sparsity: 0.26%
Layer base_model.model.out_blocks.11.attn.proj.bias sparsity: 0.46%
Layer base_model.model.out_blocks.11.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.11.attn.proj.lora_B.default.weight sparsity: 14.18%
Layer base_model.model.out_blocks.11.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.11.norm3.bias sparsity: 0.33%
Layer base_model.model.out_blocks.11.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.out_blocks.11.mlp.fc1.bias sparsity: 0.05%
Layer base_model.model.out_blocks.11.mlp.fc1.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.out_blocks.11.mlp.fc1.lora_B.default.weight sparsity: 11.41%
Layer base_model.model.out_blocks.11.mlp.fc2.weight sparsity: 0.25%
Layer base_model.model.out_blocks.11.mlp.fc2.bias sparsity: 0.52%
Layer base_model.model.out_blocks.11.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.out_blocks.11.mlp.fc2.lora_B.default.weight sparsity: 14.90%
Layer base_model.model.out_blocks.11.skip_linear.weight sparsity: 0.22%
Layer base_model.model.out_blocks.11.skip_linear.bias sparsity: 0.13%
Layer base_model.model.out_blocks.12.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.12.norm1.bias sparsity: 0.26%
Layer base_model.model.out_blocks.12.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.12.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.12.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.12.attn.qkv.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.12.attn.qkv.lora_B.default.weight sparsity: 12.43%
Layer base_model.model.out_blocks.12.attn.proj.weight sparsity: 0.24%
Layer base_model.model.out_blocks.12.attn.proj.bias sparsity: 0.52%
Layer base_model.model.out_blocks.12.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.12.attn.proj.lora_B.default.weight sparsity: 12.21%
Layer base_model.model.out_blocks.12.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.12.norm3.bias sparsity: 0.33%
Layer base_model.model.out_blocks.12.mlp.fc1.weight sparsity: 0.24%
Layer base_model.model.out_blocks.12.mlp.fc1.bias sparsity: 0.05%
Layer base_model.model.out_blocks.12.mlp.fc1.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.12.mlp.fc1.lora_B.default.weight sparsity: 11.77%
Layer base_model.model.out_blocks.12.mlp.fc2.weight sparsity: 0.29%
Layer base_model.model.out_blocks.12.mlp.fc2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.12.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.out_blocks.12.mlp.fc2.lora_B.default.weight sparsity: 15.71%
Layer base_model.model.out_blocks.12.skip_linear.weight sparsity: 0.21%
Layer base_model.model.out_blocks.12.skip_linear.bias sparsity: 0.26%
Layer base_model.model.out_blocks.13.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.13.norm1.bias sparsity: 0.07%
Layer base_model.model.out_blocks.13.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.13.norm2.bias sparsity: 0.20%
Layer base_model.model.out_blocks.13.attn.qkv.weight sparsity: 0.20%
Layer base_model.model.out_blocks.13.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.13.attn.qkv.lora_B.default.weight sparsity: 14.93%
Layer base_model.model.out_blocks.13.attn.proj.weight sparsity: 0.27%
Layer base_model.model.out_blocks.13.attn.proj.bias sparsity: 0.65%
Layer base_model.model.out_blocks.13.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.13.attn.proj.lora_B.default.weight sparsity: 14.75%
Layer base_model.model.out_blocks.13.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.13.norm3.bias sparsity: 0.52%
Layer base_model.model.out_blocks.13.mlp.fc1.weight sparsity: 0.25%
Layer base_model.model.out_blocks.13.mlp.fc1.bias sparsity: 0.02%
Layer base_model.model.out_blocks.13.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.13.mlp.fc1.lora_B.default.weight sparsity: 12.90%
Layer base_model.model.out_blocks.13.mlp.fc2.weight sparsity: 0.37%
Layer base_model.model.out_blocks.13.mlp.fc2.bias sparsity: 0.33%
Layer base_model.model.out_blocks.13.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.out_blocks.13.mlp.fc2.lora_B.default.weight sparsity: 15.10%
Layer base_model.model.out_blocks.13.skip_linear.weight sparsity: 0.21%
Layer base_model.model.out_blocks.13.skip_linear.bias sparsity: 0.20%
Layer base_model.model.out_blocks.14.norm1.weight sparsity: 0.20%
Layer base_model.model.out_blocks.14.norm1.bias sparsity: 0.33%
Layer base_model.model.out_blocks.14.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.14.norm2.bias sparsity: 0.78%
Layer base_model.model.out_blocks.14.attn.qkv.weight sparsity: 0.28%
Layer base_model.model.out_blocks.14.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.14.attn.qkv.lora_B.default.weight sparsity: 14.71%
Layer base_model.model.out_blocks.14.attn.proj.weight sparsity: 0.41%
Layer base_model.model.out_blocks.14.attn.proj.bias sparsity: 0.46%
Layer base_model.model.out_blocks.14.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.14.attn.proj.lora_B.default.weight sparsity: 16.50%
Layer base_model.model.out_blocks.14.norm3.weight sparsity: 2.54%
Layer base_model.model.out_blocks.14.norm3.bias sparsity: 7.03%
Layer base_model.model.out_blocks.14.mlp.fc1.weight sparsity: 3.41%
Layer base_model.model.out_blocks.14.mlp.fc1.bias sparsity: 2.38%
Layer base_model.model.out_blocks.14.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.14.mlp.fc1.lora_B.default.weight sparsity: 17.50%
Layer base_model.model.out_blocks.14.mlp.fc2.weight sparsity: 1.36%
Layer base_model.model.out_blocks.14.mlp.fc2.bias sparsity: 1.17%
Layer base_model.model.out_blocks.14.mlp.fc2.lora_A.default.weight sparsity: 0.80%
Layer base_model.model.out_blocks.14.mlp.fc2.lora_B.default.weight sparsity: 17.46%
Layer base_model.model.out_blocks.14.skip_linear.weight sparsity: 0.24%
Layer base_model.model.out_blocks.14.skip_linear.bias sparsity: 0.39%
Layer base_model.model.norm.weight sparsity: 3.97%
Layer base_model.model.norm.bias sparsity: 8.66%
Layer base_model.model.decoder_pred.weight sparsity: 5.39%
Layer base_model.model.decoder_pred.bias sparsity: 6.25%
Layer base_model.model.adapters_itot.0.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_q.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.0.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.0.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_k.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.0.to_k.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_itot.0.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.0.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.0.to_v.lora_B.default.weight sparsity: 61.16%
Layer base_model.model.adapters_itot.0.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_out.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.0.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.0.to_out.lora_B.default.weight sparsity: 19.99%
Layer base_model.model.adapters_itot.1.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_q.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.1.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.1.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_k.bias sparsity: 0.07%
Layer base_model.model.adapters_itot.1.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.1.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_v.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.1.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.1.to_v.lora_B.default.weight sparsity: 5.09%
Layer base_model.model.adapters_itot.1.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.1.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.1.to_out.lora_B.default.weight sparsity: 11.78%
Layer base_model.model.adapters_itot.2.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.2.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.2.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_k.bias sparsity: 0.65%
Layer base_model.model.adapters_itot.2.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.2.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.2.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.2.to_v.lora_B.default.weight sparsity: 2.51%
Layer base_model.model.adapters_itot.2.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.2.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.2.to_out.lora_B.default.weight sparsity: 11.03%
Layer base_model.model.adapters_itot.3.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_q.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.3.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.3.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.3.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.3.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.3.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.3.to_v.lora_B.default.weight sparsity: 9.57%
Layer base_model.model.adapters_itot.3.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.3.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.3.to_out.lora_B.default.weight sparsity: 14.23%
Layer base_model.model.adapters_itot.4.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.4.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.4.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_k.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.4.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.4.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.4.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.4.to_v.lora_B.default.weight sparsity: 2.40%
Layer base_model.model.adapters_itot.4.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.4.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.4.to_out.lora_B.default.weight sparsity: 10.60%
Layer base_model.model.adapters_itot.5.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.5.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.5.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_k.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.5.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.5.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_v.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.5.to_v.lora_A.default.weight sparsity: 0.44%
Layer base_model.model.adapters_itot.5.to_v.lora_B.default.weight sparsity: 1.05%
Layer base_model.model.adapters_itot.5.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_out.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.5.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.5.to_out.lora_B.default.weight sparsity: 9.86%
Layer base_model.model.adapters_itot.6.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.6.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.6.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.6.to_k.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_itot.6.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.6.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.6.to_v.lora_B.default.weight sparsity: 0.53%
Layer base_model.model.adapters_itot.6.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_out.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.6.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.6.to_out.lora_B.default.weight sparsity: 9.40%
Layer base_model.model.adapters_itot.7.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_q.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.7.to_q.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_itot.7.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_k.bias sparsity: 0.65%
Layer base_model.model.adapters_itot.7.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.7.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.7.to_v.lora_A.default.weight sparsity: 0.43%
Layer base_model.model.adapters_itot.7.to_v.lora_B.default.weight sparsity: 1.97%
Layer base_model.model.adapters_itot.7.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.7.to_out.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.7.to_out.lora_B.default.weight sparsity: 9.94%
Layer base_model.model.adapters_itot.8.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_q.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.8.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.8.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.8.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.8.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_v.bias sparsity: 0.00%
Layer base_model.model.adapters_itot.8.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.8.to_v.lora_B.default.weight sparsity: 0.97%
Layer base_model.model.adapters_itot.8.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_out.bias sparsity: 0.13%
Layer base_model.model.adapters_itot.8.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.8.to_out.lora_B.default.weight sparsity: 8.78%
Layer base_model.model.adapters_itot.9.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_q.bias sparsity: 1.04%
Layer base_model.model.adapters_itot.9.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.9.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_k.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.9.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.9.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.9.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.9.to_v.lora_B.default.weight sparsity: 1.81%
Layer base_model.model.adapters_itot.9.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.9.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.9.to_out.lora_B.default.weight sparsity: 10.09%
Layer base_model.model.adapters_itot.10.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.10.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.10.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_k.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.10.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.10.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.10.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.10.to_v.lora_B.default.weight sparsity: 5.62%
Layer base_model.model.adapters_itot.10.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_out.bias sparsity: 0.07%
Layer base_model.model.adapters_itot.10.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.10.to_out.lora_B.default.weight sparsity: 11.15%
Layer base_model.model.adapters_itot.11.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.11.to_q.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.adapters_itot.11.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.11.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.11.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.11.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.11.to_v.lora_B.default.weight sparsity: 8.21%
Layer base_model.model.adapters_itot.11.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.11.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.11.to_out.lora_B.default.weight sparsity: 10.96%
Layer base_model.model.adapters_itot.12.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_q.bias sparsity: 0.13%
Layer base_model.model.adapters_itot.12.to_q.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_itot.12.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.12.to_k.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.12.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_v.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.12.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.12.to_v.lora_B.default.weight sparsity: 42.56%
Layer base_model.model.adapters_itot.12.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_out.bias sparsity: 0.72%
Layer base_model.model.adapters_itot.12.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.12.to_out.lora_B.default.weight sparsity: 16.24%
Layer base_model.model.adapters_itot.13.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_q.bias sparsity: 0.91%
Layer base_model.model.adapters_itot.13.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.13.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.13.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.13.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.13.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.13.to_v.lora_B.default.weight sparsity: 9.98%
Layer base_model.model.adapters_itot.13.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_out.bias sparsity: 0.78%
Layer base_model.model.adapters_itot.13.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.13.to_out.lora_B.default.weight sparsity: 9.93%
Layer base_model.model.adapters_itot.14.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.14.to_q.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_itot.14.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_k.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.14.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.14.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.14.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.14.to_v.lora_B.default.weight sparsity: 39.46%
Layer base_model.model.adapters_itot.14.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.14.to_out.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.14.to_out.lora_B.default.weight sparsity: 16.63%
Layer base_model.model.adapters_itot.15.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.15.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.15.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.15.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.15.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_v.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.15.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.15.to_v.lora_B.default.weight sparsity: 56.01%
Layer base_model.model.adapters_itot.15.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.15.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.15.to_out.lora_B.default.weight sparsity: 24.91%
Layer base_model.model.adapters_itot.16.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_q.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.16.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.16.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.16.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.16.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.16.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.16.to_v.lora_B.default.weight sparsity: 21.05%
Layer base_model.model.adapters_itot.16.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_out.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.16.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.16.to_out.lora_B.default.weight sparsity: 13.10%
Layer base_model.model.adapters_itot.17.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_q.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.17.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.17.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.17.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.17.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.17.to_v.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.17.to_v.lora_B.default.weight sparsity: 23.97%
Layer base_model.model.adapters_itot.17.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.17.to_out.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.17.to_out.lora_B.default.weight sparsity: 14.63%
Layer base_model.model.adapters_itot.18.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.18.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.18.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.18.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.18.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_v.bias sparsity: 0.65%
Layer base_model.model.adapters_itot.18.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.18.to_v.lora_B.default.weight sparsity: 24.57%
Layer base_model.model.adapters_itot.18.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.18.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.18.to_out.lora_B.default.weight sparsity: 12.39%
Layer base_model.model.adapters_itot.19.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.19.to_q.lora_A.default.weight sparsity: 0.44%
Layer base_model.model.adapters_itot.19.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.19.to_k.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.19.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.19.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.19.to_v.lora_B.default.weight sparsity: 12.37%
Layer base_model.model.adapters_itot.19.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_out.bias sparsity: 0.72%
Layer base_model.model.adapters_itot.19.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.19.to_out.lora_B.default.weight sparsity: 9.66%
Layer base_model.model.adapters_itot.20.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.20.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.20.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_k.bias sparsity: 0.78%
Layer base_model.model.adapters_itot.20.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.20.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_v.bias sparsity: 0.07%
Layer base_model.model.adapters_itot.20.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.20.to_v.lora_B.default.weight sparsity: 11.00%
Layer base_model.model.adapters_itot.20.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.20.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.20.to_out.lora_B.default.weight sparsity: 11.20%
Layer base_model.model.adapters_itot.21.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.21.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.21.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.21.to_k.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.adapters_itot.21.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.21.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.21.to_v.lora_B.default.weight sparsity: 3.80%
Layer base_model.model.adapters_itot.21.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.21.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.21.to_out.lora_B.default.weight sparsity: 9.67%
Layer base_model.model.adapters_itot.22.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_q.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.22.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.22.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.22.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.22.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.22.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.22.to_v.lora_B.default.weight sparsity: 1.57%
Layer base_model.model.adapters_itot.22.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.22.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.22.to_out.lora_B.default.weight sparsity: 9.17%
Layer base_model.model.adapters_itot.23.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.23.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.23.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.23.to_k.lora_A.default.weight sparsity: 0.43%
Layer base_model.model.adapters_itot.23.to_k.lora_B.default.weight sparsity: 98.24%
Layer base_model.model.adapters_itot.23.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.23.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.23.to_v.lora_B.default.weight sparsity: 1.44%
Layer base_model.model.adapters_itot.23.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.23.to_out.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.23.to_out.lora_B.default.weight sparsity: 9.79%
Layer base_model.model.adapters_itot.24.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_q.bias sparsity: 0.65%
Layer base_model.model.adapters_itot.24.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.24.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.24.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.24.to_k.lora_B.default.weight sparsity: 82.59%
Layer base_model.model.adapters_itot.24.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_v.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.24.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.24.to_v.lora_B.default.weight sparsity: 0.76%
Layer base_model.model.adapters_itot.24.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.24.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.24.to_out.lora_B.default.weight sparsity: 7.58%
Layer base_model.model.adapters_itot.25.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_q.bias sparsity: 0.13%
Layer base_model.model.adapters_itot.25.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.25.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.25.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.25.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.25.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.25.to_v.lora_B.default.weight sparsity: 2.31%
Layer base_model.model.adapters_itot.25.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.25.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.25.to_out.lora_B.default.weight sparsity: 9.32%
Layer base_model.model.adapters_itot.26.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.26.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.26.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_k.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.26.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.26.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_v.bias sparsity: 0.13%
Layer base_model.model.adapters_itot.26.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.26.to_v.lora_B.default.weight sparsity: 3.78%
Layer base_model.model.adapters_itot.26.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.26.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.26.to_out.lora_B.default.weight sparsity: 11.53%
Layer base_model.model.adapters_itot.27.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_q.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.27.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.27.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.27.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.27.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_v.bias sparsity: 0.13%
Layer base_model.model.adapters_itot.27.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.27.to_v.lora_B.default.weight sparsity: 17.88%
Layer base_model.model.adapters_itot.27.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.27.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.27.to_out.lora_B.default.weight sparsity: 17.72%
Layer base_model.model.adapters_itot.28.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_q.bias sparsity: 0.65%
Layer base_model.model.adapters_itot.28.to_q.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_itot.28.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_k.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.28.to_k.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.28.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.28.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.28.to_v.lora_B.default.weight sparsity: 38.70%
Layer base_model.model.adapters_itot.28.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.28.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.28.to_out.lora_B.default.weight sparsity: 26.78%
Layer base_model.model.adapters_itot.29.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_q.bias sparsity: 0.13%
Layer base_model.model.adapters_itot.29.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.29.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_k.bias sparsity: 0.13%
Layer base_model.model.adapters_itot.29.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.29.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.29.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.29.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.29.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.29.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.0.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.0.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.0.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.0.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.0.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.0.to_v.lora_B.default.weight sparsity: 40.47%
Layer base_model.model.adapters_ttoi.0.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.0.to_out.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.adapters_ttoi.0.to_out.lora_B.default.weight sparsity: 20.13%
Layer base_model.model.adapters_ttoi.1.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_q.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.1.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.1.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_k.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.1.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.1.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_v.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.1.to_v.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.1.to_v.lora_B.default.weight sparsity: 10.01%
Layer base_model.model.adapters_ttoi.1.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.1.to_out.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.1.to_out.lora_B.default.weight sparsity: 17.86%
Layer base_model.model.adapters_ttoi.2.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_q.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.2.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.2.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.2.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.2.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_v.bias sparsity: 0.07%
Layer base_model.model.adapters_ttoi.2.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.2.to_v.lora_B.default.weight sparsity: 9.95%
Layer base_model.model.adapters_ttoi.2.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_out.bias sparsity: 0.07%
Layer base_model.model.adapters_ttoi.2.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.2.to_out.lora_B.default.weight sparsity: 17.82%
Layer base_model.model.adapters_ttoi.3.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_q.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.3.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.3.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.3.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.3.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.3.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.3.to_v.lora_B.default.weight sparsity: 13.65%
Layer base_model.model.adapters_ttoi.3.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.3.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.3.to_out.lora_B.default.weight sparsity: 17.33%
Layer base_model.model.adapters_ttoi.4.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_q.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.4.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.4.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_k.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.4.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.4.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.4.to_v.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.4.to_v.lora_B.default.weight sparsity: 14.21%
Layer base_model.model.adapters_ttoi.4.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_out.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.4.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.4.to_out.lora_B.default.weight sparsity: 17.12%
Layer base_model.model.adapters_ttoi.5.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_q.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.5.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.5.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.5.to_k.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.5.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_v.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.5.to_v.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.5.to_v.lora_B.default.weight sparsity: 8.95%
Layer base_model.model.adapters_ttoi.5.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.5.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.5.to_out.lora_B.default.weight sparsity: 15.83%
Layer base_model.model.adapters_ttoi.6.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.6.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.6.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.6.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.6.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_v.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.6.to_v.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.6.to_v.lora_B.default.weight sparsity: 22.06%
Layer base_model.model.adapters_ttoi.6.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.6.to_out.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.6.to_out.lora_B.default.weight sparsity: 18.63%
Layer base_model.model.adapters_ttoi.7.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_q.bias sparsity: 0.13%
Layer base_model.model.adapters_ttoi.7.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.7.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.7.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.7.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.7.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.7.to_v.lora_B.default.weight sparsity: 9.03%
Layer base_model.model.adapters_ttoi.7.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.7.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.7.to_out.lora_B.default.weight sparsity: 14.45%
Layer base_model.model.adapters_ttoi.8.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.8.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.8.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_k.bias sparsity: 0.13%
Layer base_model.model.adapters_ttoi.8.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.8.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.8.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.8.to_v.lora_B.default.weight sparsity: 6.18%
Layer base_model.model.adapters_ttoi.8.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.8.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.8.to_out.lora_B.default.weight sparsity: 15.34%
Layer base_model.model.adapters_ttoi.9.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_q.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.9.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.9.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.9.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.9.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_v.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.9.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.9.to_v.lora_B.default.weight sparsity: 5.03%
Layer base_model.model.adapters_ttoi.9.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.9.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.9.to_out.lora_B.default.weight sparsity: 13.20%
Layer base_model.model.adapters_ttoi.10.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.10.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.10.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.10.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.10.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_v.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.10.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.10.to_v.lora_B.default.weight sparsity: 0.78%
Layer base_model.model.adapters_ttoi.10.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_out.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.10.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.10.to_out.lora_B.default.weight sparsity: 12.30%
Layer base_model.model.adapters_ttoi.11.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.11.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.11.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.11.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.11.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.11.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.11.to_v.lora_B.default.weight sparsity: 0.58%
Layer base_model.model.adapters_ttoi.11.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.11.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.11.to_out.lora_B.default.weight sparsity: 10.40%
Layer base_model.model.adapters_ttoi.12.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.12.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.12.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.12.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.12.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_v.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.12.to_v.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.12.to_v.lora_B.default.weight sparsity: 0.18%
Layer base_model.model.adapters_ttoi.12.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.12.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.12.to_out.lora_B.default.weight sparsity: 9.54%
Layer base_model.model.adapters_ttoi.13.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_q.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.13.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.13.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.13.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.13.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.13.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.13.to_v.lora_B.default.weight sparsity: 0.14%
Layer base_model.model.adapters_ttoi.13.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.13.to_out.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.adapters_ttoi.13.to_out.lora_B.default.weight sparsity: 10.39%
Layer base_model.model.adapters_ttoi.14.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.14.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.14.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.14.to_k.lora_A.default.weight sparsity: 0.35%
Layer base_model.model.adapters_ttoi.14.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.14.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.14.to_v.lora_B.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.14.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_out.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.14.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.14.to_out.lora_B.default.weight sparsity: 9.62%
Layer base_model.model.adapters_ttoi.15.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_q.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.15.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.15.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.15.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.15.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.15.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.15.to_v.lora_B.default.weight sparsity: 1.92%
Layer base_model.model.adapters_ttoi.15.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.15.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.15.to_out.lora_B.default.weight sparsity: 11.12%
Layer base_model.model.adapters_ttoi.16.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.16.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.16.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.16.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.16.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.16.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.16.to_v.lora_B.default.weight sparsity: 0.07%
Layer base_model.model.adapters_ttoi.16.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_out.bias sparsity: 0.13%
Layer base_model.model.adapters_ttoi.16.to_out.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.16.to_out.lora_B.default.weight sparsity: 9.23%
Layer base_model.model.adapters_ttoi.17.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_q.bias sparsity: 0.07%
Layer base_model.model.adapters_ttoi.17.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.17.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.17.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.17.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.17.to_v.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_ttoi.17.to_v.lora_B.default.weight sparsity: 0.04%
Layer base_model.model.adapters_ttoi.17.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_out.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.17.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.17.to_out.lora_B.default.weight sparsity: 8.35%
Layer base_model.model.adapters_ttoi.18.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.18.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.18.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.18.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.18.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.18.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.18.to_v.lora_B.default.weight sparsity: 0.88%
Layer base_model.model.adapters_ttoi.18.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_out.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.18.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.18.to_out.lora_B.default.weight sparsity: 11.36%
Layer base_model.model.adapters_ttoi.19.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_q.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.19.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.19.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.19.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.19.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.19.to_v.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.19.to_v.lora_B.default.weight sparsity: 0.73%
Layer base_model.model.adapters_ttoi.19.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.19.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.19.to_out.lora_B.default.weight sparsity: 11.36%
Layer base_model.model.adapters_ttoi.20.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_q.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.20.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.20.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.20.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.20.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_v.bias sparsity: 0.78%
Layer base_model.model.adapters_ttoi.20.to_v.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.20.to_v.lora_B.default.weight sparsity: 2.84%
Layer base_model.model.adapters_ttoi.20.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.20.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.20.to_out.lora_B.default.weight sparsity: 14.94%
Layer base_model.model.adapters_ttoi.21.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_q.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.21.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.21.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_k.bias sparsity: 0.00%
Layer base_model.model.adapters_ttoi.21.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.21.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_v.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.21.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.21.to_v.lora_B.default.weight sparsity: 11.95%
Layer base_model.model.adapters_ttoi.21.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.21.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.21.to_out.lora_B.default.weight sparsity: 17.81%
Layer base_model.model.adapters_ttoi.22.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_q.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.22.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.22.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_k.bias sparsity: 0.72%
Layer base_model.model.adapters_ttoi.22.to_k.lora_A.default.weight sparsity: 0.44%
Layer base_model.model.adapters_ttoi.22.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.22.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.22.to_v.lora_B.default.weight sparsity: 15.97%
Layer base_model.model.adapters_ttoi.22.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.22.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.22.to_out.lora_B.default.weight sparsity: 22.84%
Layer base_model.model.adapters_ttoi.23.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_q.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.23.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.23.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_k.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.23.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.23.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.23.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.23.to_v.lora_B.default.weight sparsity: 32.53%
Layer base_model.model.adapters_ttoi.23.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_out.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.23.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.23.to_out.lora_B.default.weight sparsity: 24.80%
Layer base_model.model.adapters_ttoi.24.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_q.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.24.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.24.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.24.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.24.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.24.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.24.to_v.lora_B.default.weight sparsity: 27.44%
Layer base_model.model.adapters_ttoi.24.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.24.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.24.to_out.lora_B.default.weight sparsity: 25.98%
Layer base_model.model.adapters_ttoi.25.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_q.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.25.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.25.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.25.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.25.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.25.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.25.to_v.lora_B.default.weight sparsity: 15.86%
Layer base_model.model.adapters_ttoi.25.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.25.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.25.to_out.lora_B.default.weight sparsity: 22.89%
Layer base_model.model.adapters_ttoi.26.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.26.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.26.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.26.to_k.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.26.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.26.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.26.to_v.lora_B.default.weight sparsity: 3.90%
Layer base_model.model.adapters_ttoi.26.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_out.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.26.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.26.to_out.lora_B.default.weight sparsity: 12.20%
Layer base_model.model.adapters_ttoi.27.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_q.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.27.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.27.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.27.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.27.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.27.to_v.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.27.to_v.lora_B.default.weight sparsity: 11.94%
Layer base_model.model.adapters_ttoi.27.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_out.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.27.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.27.to_out.lora_B.default.weight sparsity: 21.42%
Layer base_model.model.adapters_ttoi.28.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_q.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.28.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.28.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.28.to_k.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.adapters_ttoi.28.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_v.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.28.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.28.to_v.lora_B.default.weight sparsity: 22.81%
Layer base_model.model.adapters_ttoi.28.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_out.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.28.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.28.to_out.lora_B.default.weight sparsity: 28.93%
Layer base_model.model.adapters_ttoi.29.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.29.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.29.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.29.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.29.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.29.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.29.to_v.lora_B.default.weight sparsity: 26.46%
Layer base_model.model.adapters_ttoi.29.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.29.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.29.to_out.lora_B.default.weight sparsity: 16.65%
Layer base_model.model.token_embedding.weight sparsity: 0.00%
Total sparsity: 35.34%
lora 192555008
Total parameters: 1711664208
Layer base_model.model.pos_embed sparsity: 0.28%
Layer base_model.model.pos_embed_token sparsity: 1.11%
Layer base_model.model.patch_embed.proj.weight sparsity: 0.29%
Layer base_model.model.patch_embed.proj.bias sparsity: 0.78%
Layer base_model.model.patch_embed.proj.lora_A.default.weight sparsity: 0.05%
Layer base_model.model.patch_embed.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.text_embed.weight sparsity: 0.49%
Layer base_model.model.text_embed.bias sparsity: 0.52%
Layer base_model.model.text_embed.lora_A.default.weight sparsity: 0.06%
Layer base_model.model.text_embed.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.text_out.weight sparsity: 4.68%
Layer base_model.model.text_out.bias sparsity: 10.94%
Layer base_model.model.clip_img_embed.weight sparsity: 0.32%
Layer base_model.model.clip_img_embed.bias sparsity: 0.00%
Layer base_model.model.clip_img_embed.lora_A.default.weight sparsity: 0.26%
Layer base_model.model.clip_img_embed.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.clip_img_out.weight sparsity: 3.63%
Layer base_model.model.clip_img_out.bias sparsity: 4.30%
Layer base_model.model.in_blocks.0.norm2.weight sparsity: 0.13%
Layer base_model.model.in_blocks.0.norm2.bias sparsity: 0.46%
Layer base_model.model.in_blocks.0.attn.qkv.weight sparsity: 0.75%
Layer base_model.model.in_blocks.0.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.0.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.0.attn.proj.weight sparsity: 1.20%
Layer base_model.model.in_blocks.0.attn.proj.bias sparsity: 0.00%
Layer base_model.model.in_blocks.0.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.0.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.0.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.0.norm3.bias sparsity: 0.52%
Layer base_model.model.in_blocks.0.mlp.fc1.weight sparsity: 0.28%
Layer base_model.model.in_blocks.0.mlp.fc1.bias sparsity: 0.52%
Layer base_model.model.in_blocks.0.mlp.fc1.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.in_blocks.0.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.0.mlp.fc2.weight sparsity: 0.31%
Layer base_model.model.in_blocks.0.mlp.fc2.bias sparsity: 0.46%
Layer base_model.model.in_blocks.0.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.in_blocks.0.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.1.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.1.norm2.bias sparsity: 0.13%
Layer base_model.model.in_blocks.1.attn.qkv.weight sparsity: 0.28%
Layer base_model.model.in_blocks.1.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.1.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.1.attn.proj.weight sparsity: 0.43%
Layer base_model.model.in_blocks.1.attn.proj.bias sparsity: 0.78%
Layer base_model.model.in_blocks.1.attn.proj.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.1.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.1.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.1.norm3.bias sparsity: 0.85%
Layer base_model.model.in_blocks.1.mlp.fc1.weight sparsity: 0.28%
Layer base_model.model.in_blocks.1.mlp.fc1.bias sparsity: 0.08%
Layer base_model.model.in_blocks.1.mlp.fc1.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.1.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.1.mlp.fc2.weight sparsity: 0.39%
Layer base_model.model.in_blocks.1.mlp.fc2.bias sparsity: 0.52%
Layer base_model.model.in_blocks.1.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.in_blocks.1.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.2.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.2.norm2.bias sparsity: 0.07%
Layer base_model.model.in_blocks.2.attn.qkv.weight sparsity: 0.23%
Layer base_model.model.in_blocks.2.attn.qkv.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.2.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.2.attn.proj.weight sparsity: 0.28%
Layer base_model.model.in_blocks.2.attn.proj.bias sparsity: 0.72%
Layer base_model.model.in_blocks.2.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.2.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.2.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.2.norm3.bias sparsity: 1.04%
Layer base_model.model.in_blocks.2.mlp.fc1.weight sparsity: 0.26%
Layer base_model.model.in_blocks.2.mlp.fc1.bias sparsity: 0.11%
Layer base_model.model.in_blocks.2.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.2.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.2.mlp.fc2.weight sparsity: 0.39%
Layer base_model.model.in_blocks.2.mlp.fc2.bias sparsity: 0.46%
Layer base_model.model.in_blocks.2.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.in_blocks.2.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.3.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.3.norm2.bias sparsity: 0.07%
Layer base_model.model.in_blocks.3.attn.qkv.weight sparsity: 0.22%
Layer base_model.model.in_blocks.3.attn.qkv.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.3.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.3.attn.proj.weight sparsity: 0.29%
Layer base_model.model.in_blocks.3.attn.proj.bias sparsity: 0.59%
Layer base_model.model.in_blocks.3.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.3.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.3.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.3.norm3.bias sparsity: 0.65%
Layer base_model.model.in_blocks.3.mlp.fc1.weight sparsity: 0.21%
Layer base_model.model.in_blocks.3.mlp.fc1.bias sparsity: 0.29%
Layer base_model.model.in_blocks.3.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.3.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.3.mlp.fc2.weight sparsity: 0.25%
Layer base_model.model.in_blocks.3.mlp.fc2.bias sparsity: 0.52%
Layer base_model.model.in_blocks.3.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.in_blocks.3.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.4.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.4.norm2.bias sparsity: 0.00%
Layer base_model.model.in_blocks.4.attn.qkv.weight sparsity: 0.21%
Layer base_model.model.in_blocks.4.attn.qkv.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.4.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.4.attn.proj.weight sparsity: 0.26%
Layer base_model.model.in_blocks.4.attn.proj.bias sparsity: 0.59%
Layer base_model.model.in_blocks.4.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.4.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.4.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.4.norm3.bias sparsity: 0.39%
Layer base_model.model.in_blocks.4.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.4.mlp.fc1.bias sparsity: 0.23%
Layer base_model.model.in_blocks.4.mlp.fc1.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.in_blocks.4.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.4.mlp.fc2.weight sparsity: 0.23%
Layer base_model.model.in_blocks.4.mlp.fc2.bias sparsity: 0.72%
Layer base_model.model.in_blocks.4.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.in_blocks.4.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.5.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.5.norm2.bias sparsity: 0.07%
Layer base_model.model.in_blocks.5.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.in_blocks.5.attn.qkv.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.5.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.5.attn.proj.weight sparsity: 0.23%
Layer base_model.model.in_blocks.5.attn.proj.bias sparsity: 1.30%
Layer base_model.model.in_blocks.5.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.5.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.5.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.5.norm3.bias sparsity: 0.13%
Layer base_model.model.in_blocks.5.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.5.mlp.fc1.bias sparsity: 0.13%
Layer base_model.model.in_blocks.5.mlp.fc1.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.5.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.5.mlp.fc2.weight sparsity: 0.23%
Layer base_model.model.in_blocks.5.mlp.fc2.bias sparsity: 0.26%
Layer base_model.model.in_blocks.5.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.in_blocks.5.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.6.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.6.norm2.bias sparsity: 0.13%
Layer base_model.model.in_blocks.6.attn.qkv.weight sparsity: 0.17%
Layer base_model.model.in_blocks.6.attn.qkv.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.6.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.6.attn.proj.weight sparsity: 0.23%
Layer base_model.model.in_blocks.6.attn.proj.bias sparsity: 0.65%
Layer base_model.model.in_blocks.6.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.6.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.6.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.6.norm3.bias sparsity: 0.26%
Layer base_model.model.in_blocks.6.mlp.fc1.weight sparsity: 0.19%
Layer base_model.model.in_blocks.6.mlp.fc1.bias sparsity: 0.10%
Layer base_model.model.in_blocks.6.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.6.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.6.mlp.fc2.weight sparsity: 0.22%
Layer base_model.model.in_blocks.6.mlp.fc2.bias sparsity: 0.46%
Layer base_model.model.in_blocks.6.mlp.fc2.lora_A.default.weight sparsity: 0.80%
Layer base_model.model.in_blocks.6.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.7.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.7.norm2.bias sparsity: 0.13%
Layer base_model.model.in_blocks.7.attn.qkv.weight sparsity: 0.17%
Layer base_model.model.in_blocks.7.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.7.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.7.attn.proj.weight sparsity: 0.25%
Layer base_model.model.in_blocks.7.attn.proj.bias sparsity: 0.59%
Layer base_model.model.in_blocks.7.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.7.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.7.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.7.norm3.bias sparsity: 0.07%
Layer base_model.model.in_blocks.7.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.7.mlp.fc1.bias sparsity: 0.28%
Layer base_model.model.in_blocks.7.mlp.fc1.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.in_blocks.7.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.7.mlp.fc2.weight sparsity: 0.24%
Layer base_model.model.in_blocks.7.mlp.fc2.bias sparsity: 0.46%
Layer base_model.model.in_blocks.7.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.in_blocks.7.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.8.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.8.norm2.bias sparsity: 0.07%
Layer base_model.model.in_blocks.8.attn.qkv.weight sparsity: 0.18%
Layer base_model.model.in_blocks.8.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.8.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.8.attn.proj.weight sparsity: 0.25%
Layer base_model.model.in_blocks.8.attn.proj.bias sparsity: 0.91%
Layer base_model.model.in_blocks.8.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.8.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.8.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.8.norm3.bias sparsity: 0.13%
Layer base_model.model.in_blocks.8.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.8.mlp.fc1.bias sparsity: 0.23%
Layer base_model.model.in_blocks.8.mlp.fc1.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.8.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.8.mlp.fc2.weight sparsity: 0.24%
Layer base_model.model.in_blocks.8.mlp.fc2.bias sparsity: 0.39%
Layer base_model.model.in_blocks.8.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.in_blocks.8.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.9.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.9.norm2.bias sparsity: 0.13%
Layer base_model.model.in_blocks.9.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.in_blocks.9.attn.qkv.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.in_blocks.9.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.9.attn.proj.weight sparsity: 0.27%
Layer base_model.model.in_blocks.9.attn.proj.bias sparsity: 0.72%
Layer base_model.model.in_blocks.9.attn.proj.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.9.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.9.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.9.norm3.bias sparsity: 0.39%
Layer base_model.model.in_blocks.9.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.9.mlp.fc1.bias sparsity: 0.31%
Layer base_model.model.in_blocks.9.mlp.fc1.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.9.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.9.mlp.fc2.weight sparsity: 0.23%
Layer base_model.model.in_blocks.9.mlp.fc2.bias sparsity: 0.33%
Layer base_model.model.in_blocks.9.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.in_blocks.9.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.10.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.10.norm2.bias sparsity: 0.13%
Layer base_model.model.in_blocks.10.attn.qkv.weight sparsity: 0.18%
Layer base_model.model.in_blocks.10.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.10.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.10.attn.proj.weight sparsity: 0.26%
Layer base_model.model.in_blocks.10.attn.proj.bias sparsity: 0.39%
Layer base_model.model.in_blocks.10.attn.proj.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.10.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.10.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.10.norm3.bias sparsity: 0.52%
Layer base_model.model.in_blocks.10.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.10.mlp.fc1.bias sparsity: 0.28%
Layer base_model.model.in_blocks.10.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.10.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.10.mlp.fc2.weight sparsity: 0.23%
Layer base_model.model.in_blocks.10.mlp.fc2.bias sparsity: 0.39%
Layer base_model.model.in_blocks.10.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.in_blocks.10.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.11.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.11.norm2.bias sparsity: 0.20%
Layer base_model.model.in_blocks.11.attn.qkv.weight sparsity: 0.18%
Layer base_model.model.in_blocks.11.attn.qkv.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.in_blocks.11.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.11.attn.proj.weight sparsity: 0.25%
Layer base_model.model.in_blocks.11.attn.proj.bias sparsity: 0.52%
Layer base_model.model.in_blocks.11.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.11.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.11.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.11.norm3.bias sparsity: 0.39%
Layer base_model.model.in_blocks.11.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.11.mlp.fc1.bias sparsity: 0.37%
Layer base_model.model.in_blocks.11.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.11.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.11.mlp.fc2.weight sparsity: 0.23%
Layer base_model.model.in_blocks.11.mlp.fc2.bias sparsity: 0.98%
Layer base_model.model.in_blocks.11.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.in_blocks.11.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.12.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.12.norm2.bias sparsity: 0.00%
Layer base_model.model.in_blocks.12.attn.qkv.weight sparsity: 0.18%
Layer base_model.model.in_blocks.12.attn.qkv.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.in_blocks.12.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.12.attn.proj.weight sparsity: 0.23%
Layer base_model.model.in_blocks.12.attn.proj.bias sparsity: 0.13%
Layer base_model.model.in_blocks.12.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.12.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.12.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.12.norm3.bias sparsity: 0.07%
Layer base_model.model.in_blocks.12.mlp.fc1.weight sparsity: 0.19%
Layer base_model.model.in_blocks.12.mlp.fc1.bias sparsity: 0.52%
Layer base_model.model.in_blocks.12.mlp.fc1.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.12.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.12.mlp.fc2.weight sparsity: 0.21%
Layer base_model.model.in_blocks.12.mlp.fc2.bias sparsity: 0.78%
Layer base_model.model.in_blocks.12.mlp.fc2.lora_A.default.weight sparsity: 0.77%
Layer base_model.model.in_blocks.12.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.13.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.13.norm2.bias sparsity: 0.07%
Layer base_model.model.in_blocks.13.attn.qkv.weight sparsity: 0.18%
Layer base_model.model.in_blocks.13.attn.qkv.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.13.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.13.attn.proj.weight sparsity: 0.24%
Layer base_model.model.in_blocks.13.attn.proj.bias sparsity: 0.26%
Layer base_model.model.in_blocks.13.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.in_blocks.13.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.13.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.13.norm3.bias sparsity: 0.33%
Layer base_model.model.in_blocks.13.mlp.fc1.weight sparsity: 0.19%
Layer base_model.model.in_blocks.13.mlp.fc1.bias sparsity: 0.20%
Layer base_model.model.in_blocks.13.mlp.fc1.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.in_blocks.13.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.13.mlp.fc2.weight sparsity: 0.22%
Layer base_model.model.in_blocks.13.mlp.fc2.bias sparsity: 0.39%
Layer base_model.model.in_blocks.13.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.in_blocks.13.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.14.norm2.weight sparsity: 0.00%
Layer base_model.model.in_blocks.14.norm2.bias sparsity: 0.07%
Layer base_model.model.in_blocks.14.attn.qkv.weight sparsity: 0.20%
Layer base_model.model.in_blocks.14.attn.qkv.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.in_blocks.14.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.14.attn.proj.weight sparsity: 0.25%
Layer base_model.model.in_blocks.14.attn.proj.bias sparsity: 0.72%
Layer base_model.model.in_blocks.14.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.in_blocks.14.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.14.norm3.weight sparsity: 0.00%
Layer base_model.model.in_blocks.14.norm3.bias sparsity: 0.07%
Layer base_model.model.in_blocks.14.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.in_blocks.14.mlp.fc1.bias sparsity: 0.10%
Layer base_model.model.in_blocks.14.mlp.fc1.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.in_blocks.14.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.in_blocks.14.mlp.fc2.weight sparsity: 0.23%
Layer base_model.model.in_blocks.14.mlp.fc2.bias sparsity: 0.52%
Layer base_model.model.in_blocks.14.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.in_blocks.14.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.mid_block.norm2.weight sparsity: 0.00%
Layer base_model.model.mid_block.norm2.bias sparsity: 0.00%
Layer base_model.model.mid_block.attn.qkv.weight sparsity: 0.23%
Layer base_model.model.mid_block.attn.qkv.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.mid_block.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.mid_block.attn.proj.weight sparsity: 0.27%
Layer base_model.model.mid_block.attn.proj.bias sparsity: 0.20%
Layer base_model.model.mid_block.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.mid_block.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.mid_block.norm3.weight sparsity: 0.00%
Layer base_model.model.mid_block.norm3.bias sparsity: 0.33%
Layer base_model.model.mid_block.mlp.fc1.weight sparsity: 0.22%
Layer base_model.model.mid_block.mlp.fc1.bias sparsity: 0.34%
Layer base_model.model.mid_block.mlp.fc1.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.mid_block.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.mid_block.mlp.fc2.weight sparsity: 0.26%
Layer base_model.model.mid_block.mlp.fc2.bias sparsity: 0.72%
Layer base_model.model.mid_block.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.mid_block.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.0.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.0.norm1.bias sparsity: 0.20%
Layer base_model.model.out_blocks.0.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.0.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.0.attn.qkv.weight sparsity: 0.28%
Layer base_model.model.out_blocks.0.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.0.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.0.attn.proj.weight sparsity: 0.29%
Layer base_model.model.out_blocks.0.attn.proj.bias sparsity: 0.20%
Layer base_model.model.out_blocks.0.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.0.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.0.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.0.norm3.bias sparsity: 0.91%
Layer base_model.model.out_blocks.0.mlp.fc1.weight sparsity: 0.24%
Layer base_model.model.out_blocks.0.mlp.fc1.bias sparsity: 0.33%
Layer base_model.model.out_blocks.0.mlp.fc1.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.0.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.0.mlp.fc2.weight sparsity: 0.31%
Layer base_model.model.out_blocks.0.mlp.fc2.bias sparsity: 0.91%
Layer base_model.model.out_blocks.0.mlp.fc2.lora_A.default.weight sparsity: 0.80%
Layer base_model.model.out_blocks.0.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.0.skip_linear.weight sparsity: 0.27%
Layer base_model.model.out_blocks.0.skip_linear.bias sparsity: 0.13%
Layer base_model.model.out_blocks.1.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.1.norm1.bias sparsity: 0.33%
Layer base_model.model.out_blocks.1.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.1.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.1.attn.qkv.weight sparsity: 0.21%
Layer base_model.model.out_blocks.1.attn.qkv.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.1.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.1.attn.proj.weight sparsity: 0.24%
Layer base_model.model.out_blocks.1.attn.proj.bias sparsity: 0.13%
Layer base_model.model.out_blocks.1.attn.proj.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.out_blocks.1.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.1.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.1.norm3.bias sparsity: 0.39%
Layer base_model.model.out_blocks.1.mlp.fc1.weight sparsity: 0.19%
Layer base_model.model.out_blocks.1.mlp.fc1.bias sparsity: 0.31%
Layer base_model.model.out_blocks.1.mlp.fc1.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.1.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.1.mlp.fc2.weight sparsity: 0.22%
Layer base_model.model.out_blocks.1.mlp.fc2.bias sparsity: 0.65%
Layer base_model.model.out_blocks.1.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.out_blocks.1.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.1.skip_linear.weight sparsity: 0.22%
Layer base_model.model.out_blocks.1.skip_linear.bias sparsity: 1.11%
Layer base_model.model.out_blocks.2.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.2.norm1.bias sparsity: 0.65%
Layer base_model.model.out_blocks.2.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.2.norm2.bias sparsity: 0.00%
Layer base_model.model.out_blocks.2.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.2.attn.qkv.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.2.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.2.attn.proj.weight sparsity: 0.24%
Layer base_model.model.out_blocks.2.attn.proj.bias sparsity: 0.07%
Layer base_model.model.out_blocks.2.attn.proj.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.2.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.2.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.2.norm3.bias sparsity: 0.33%
Layer base_model.model.out_blocks.2.mlp.fc1.weight sparsity: 0.19%
Layer base_model.model.out_blocks.2.mlp.fc1.bias sparsity: 0.31%
Layer base_model.model.out_blocks.2.mlp.fc1.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.2.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.2.mlp.fc2.weight sparsity: 0.22%
Layer base_model.model.out_blocks.2.mlp.fc2.bias sparsity: 1.04%
Layer base_model.model.out_blocks.2.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.out_blocks.2.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.2.skip_linear.weight sparsity: 0.20%
Layer base_model.model.out_blocks.2.skip_linear.bias sparsity: 0.91%
Layer base_model.model.out_blocks.3.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.3.norm1.bias sparsity: 0.07%
Layer base_model.model.out_blocks.3.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.3.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.3.attn.qkv.weight sparsity: 0.20%
Layer base_model.model.out_blocks.3.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.3.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.3.attn.proj.weight sparsity: 0.28%
Layer base_model.model.out_blocks.3.attn.proj.bias sparsity: 0.59%
Layer base_model.model.out_blocks.3.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.3.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.3.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.3.norm3.bias sparsity: 0.26%
Layer base_model.model.out_blocks.3.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.out_blocks.3.mlp.fc1.bias sparsity: 0.42%
Layer base_model.model.out_blocks.3.mlp.fc1.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.out_blocks.3.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.3.mlp.fc2.weight sparsity: 0.24%
Layer base_model.model.out_blocks.3.mlp.fc2.bias sparsity: 0.33%
Layer base_model.model.out_blocks.3.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.out_blocks.3.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.3.skip_linear.weight sparsity: 0.20%
Layer base_model.model.out_blocks.3.skip_linear.bias sparsity: 0.52%
Layer base_model.model.out_blocks.4.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.4.norm1.bias sparsity: 0.13%
Layer base_model.model.out_blocks.4.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.4.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.4.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.4.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.4.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.4.attn.proj.weight sparsity: 0.28%
Layer base_model.model.out_blocks.4.attn.proj.bias sparsity: 0.46%
Layer base_model.model.out_blocks.4.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.4.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.4.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.4.norm3.bias sparsity: 0.26%
Layer base_model.model.out_blocks.4.mlp.fc1.weight sparsity: 0.21%
Layer base_model.model.out_blocks.4.mlp.fc1.bias sparsity: 0.34%
Layer base_model.model.out_blocks.4.mlp.fc1.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.4.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.4.mlp.fc2.weight sparsity: 0.26%
Layer base_model.model.out_blocks.4.mlp.fc2.bias sparsity: 0.07%
Layer base_model.model.out_blocks.4.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.out_blocks.4.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.4.skip_linear.weight sparsity: 0.22%
Layer base_model.model.out_blocks.4.skip_linear.bias sparsity: 0.65%
Layer base_model.model.out_blocks.5.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.5.norm1.bias sparsity: 0.26%
Layer base_model.model.out_blocks.5.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.5.norm2.bias sparsity: 0.20%
Layer base_model.model.out_blocks.5.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.5.attn.qkv.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.5.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.5.attn.proj.weight sparsity: 0.26%
Layer base_model.model.out_blocks.5.attn.proj.bias sparsity: 0.46%
Layer base_model.model.out_blocks.5.attn.proj.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.5.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.5.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.5.norm3.bias sparsity: 0.20%
Layer base_model.model.out_blocks.5.mlp.fc1.weight sparsity: 0.21%
Layer base_model.model.out_blocks.5.mlp.fc1.bias sparsity: 0.39%
Layer base_model.model.out_blocks.5.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.5.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.5.mlp.fc2.weight sparsity: 0.25%
Layer base_model.model.out_blocks.5.mlp.fc2.bias sparsity: 0.46%
Layer base_model.model.out_blocks.5.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.out_blocks.5.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.5.skip_linear.weight sparsity: 0.23%
Layer base_model.model.out_blocks.5.skip_linear.bias sparsity: 0.72%
Layer base_model.model.out_blocks.6.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.6.norm1.bias sparsity: 0.26%
Layer base_model.model.out_blocks.6.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.6.norm2.bias sparsity: 0.07%
Layer base_model.model.out_blocks.6.attn.qkv.weight sparsity: 0.20%
Layer base_model.model.out_blocks.6.attn.qkv.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.6.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.6.attn.proj.weight sparsity: 0.28%
Layer base_model.model.out_blocks.6.attn.proj.bias sparsity: 0.07%
Layer base_model.model.out_blocks.6.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.6.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.6.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.6.norm3.bias sparsity: 0.39%
Layer base_model.model.out_blocks.6.mlp.fc1.weight sparsity: 0.21%
Layer base_model.model.out_blocks.6.mlp.fc1.bias sparsity: 0.31%
Layer base_model.model.out_blocks.6.mlp.fc1.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.6.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.6.mlp.fc2.weight sparsity: 0.27%
Layer base_model.model.out_blocks.6.mlp.fc2.bias sparsity: 0.52%
Layer base_model.model.out_blocks.6.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.out_blocks.6.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.6.skip_linear.weight sparsity: 0.23%
Layer base_model.model.out_blocks.6.skip_linear.bias sparsity: 0.26%
Layer base_model.model.out_blocks.7.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.7.norm1.bias sparsity: 0.07%
Layer base_model.model.out_blocks.7.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.7.norm2.bias sparsity: 0.07%
Layer base_model.model.out_blocks.7.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.7.attn.qkv.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.out_blocks.7.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.7.attn.proj.weight sparsity: 0.27%
Layer base_model.model.out_blocks.7.attn.proj.bias sparsity: 0.59%
Layer base_model.model.out_blocks.7.attn.proj.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.out_blocks.7.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.7.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.7.norm3.bias sparsity: 0.13%
Layer base_model.model.out_blocks.7.mlp.fc1.weight sparsity: 0.21%
Layer base_model.model.out_blocks.7.mlp.fc1.bias sparsity: 0.39%
Layer base_model.model.out_blocks.7.mlp.fc1.lora_A.default.weight sparsity: 0.43%
Layer base_model.model.out_blocks.7.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.7.mlp.fc2.weight sparsity: 0.27%
Layer base_model.model.out_blocks.7.mlp.fc2.bias sparsity: 0.20%
Layer base_model.model.out_blocks.7.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.out_blocks.7.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.7.skip_linear.weight sparsity: 0.25%
Layer base_model.model.out_blocks.7.skip_linear.bias sparsity: 0.78%
Layer base_model.model.out_blocks.8.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.8.norm1.bias sparsity: 0.13%
Layer base_model.model.out_blocks.8.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.8.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.8.attn.qkv.weight sparsity: 0.20%
Layer base_model.model.out_blocks.8.attn.qkv.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.out_blocks.8.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.8.attn.proj.weight sparsity: 0.31%
Layer base_model.model.out_blocks.8.attn.proj.bias sparsity: 0.72%
Layer base_model.model.out_blocks.8.attn.proj.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.8.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.8.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.8.norm3.bias sparsity: 0.33%
Layer base_model.model.out_blocks.8.mlp.fc1.weight sparsity: 0.21%
Layer base_model.model.out_blocks.8.mlp.fc1.bias sparsity: 0.08%
Layer base_model.model.out_blocks.8.mlp.fc1.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.8.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.8.mlp.fc2.weight sparsity: 0.28%
Layer base_model.model.out_blocks.8.mlp.fc2.bias sparsity: 0.65%
Layer base_model.model.out_blocks.8.mlp.fc2.lora_A.default.weight sparsity: 0.78%
Layer base_model.model.out_blocks.8.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.8.skip_linear.weight sparsity: 0.29%
Layer base_model.model.out_blocks.8.skip_linear.bias sparsity: 0.46%
Layer base_model.model.out_blocks.9.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.9.norm1.bias sparsity: 0.00%
Layer base_model.model.out_blocks.9.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.9.norm2.bias sparsity: 0.07%
Layer base_model.model.out_blocks.9.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.9.attn.qkv.lora_A.default.weight sparsity: 0.44%
Layer base_model.model.out_blocks.9.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.9.attn.proj.weight sparsity: 0.29%
Layer base_model.model.out_blocks.9.attn.proj.bias sparsity: 0.59%
Layer base_model.model.out_blocks.9.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.9.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.9.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.9.norm3.bias sparsity: 0.33%
Layer base_model.model.out_blocks.9.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.out_blocks.9.mlp.fc1.bias sparsity: 0.26%
Layer base_model.model.out_blocks.9.mlp.fc1.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.out_blocks.9.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.9.mlp.fc2.weight sparsity: 0.26%
Layer base_model.model.out_blocks.9.mlp.fc2.bias sparsity: 0.65%
Layer base_model.model.out_blocks.9.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.out_blocks.9.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.9.skip_linear.weight sparsity: 0.32%
Layer base_model.model.out_blocks.9.skip_linear.bias sparsity: 0.26%
Layer base_model.model.out_blocks.10.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.10.norm1.bias sparsity: 0.07%
Layer base_model.model.out_blocks.10.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.10.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.10.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.10.attn.qkv.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.10.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.10.attn.proj.weight sparsity: 0.28%
Layer base_model.model.out_blocks.10.attn.proj.bias sparsity: 0.46%
Layer base_model.model.out_blocks.10.attn.proj.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.10.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.10.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.10.norm3.bias sparsity: 0.26%
Layer base_model.model.out_blocks.10.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.out_blocks.10.mlp.fc1.bias sparsity: 0.24%
Layer base_model.model.out_blocks.10.mlp.fc1.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.out_blocks.10.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.10.mlp.fc2.weight sparsity: 0.24%
Layer base_model.model.out_blocks.10.mlp.fc2.bias sparsity: 0.98%
Layer base_model.model.out_blocks.10.mlp.fc2.lora_A.default.weight sparsity: 0.80%
Layer base_model.model.out_blocks.10.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.10.skip_linear.weight sparsity: 0.25%
Layer base_model.model.out_blocks.10.skip_linear.bias sparsity: 0.39%
Layer base_model.model.out_blocks.11.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.11.norm1.bias sparsity: 0.20%
Layer base_model.model.out_blocks.11.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.11.norm2.bias sparsity: 0.07%
Layer base_model.model.out_blocks.11.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.11.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.11.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.11.attn.proj.weight sparsity: 0.26%
Layer base_model.model.out_blocks.11.attn.proj.bias sparsity: 0.46%
Layer base_model.model.out_blocks.11.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.11.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.11.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.11.norm3.bias sparsity: 0.33%
Layer base_model.model.out_blocks.11.mlp.fc1.weight sparsity: 0.20%
Layer base_model.model.out_blocks.11.mlp.fc1.bias sparsity: 0.05%
Layer base_model.model.out_blocks.11.mlp.fc1.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.11.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.11.mlp.fc2.weight sparsity: 0.25%
Layer base_model.model.out_blocks.11.mlp.fc2.bias sparsity: 0.52%
Layer base_model.model.out_blocks.11.mlp.fc2.lora_A.default.weight sparsity: 0.77%
Layer base_model.model.out_blocks.11.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.11.skip_linear.weight sparsity: 0.22%
Layer base_model.model.out_blocks.11.skip_linear.bias sparsity: 0.13%
Layer base_model.model.out_blocks.12.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.12.norm1.bias sparsity: 0.26%
Layer base_model.model.out_blocks.12.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.12.norm2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.12.attn.qkv.weight sparsity: 0.19%
Layer base_model.model.out_blocks.12.attn.qkv.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.12.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.12.attn.proj.weight sparsity: 0.24%
Layer base_model.model.out_blocks.12.attn.proj.bias sparsity: 0.52%
Layer base_model.model.out_blocks.12.attn.proj.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.out_blocks.12.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.12.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.12.norm3.bias sparsity: 0.33%
Layer base_model.model.out_blocks.12.mlp.fc1.weight sparsity: 0.24%
Layer base_model.model.out_blocks.12.mlp.fc1.bias sparsity: 0.05%
Layer base_model.model.out_blocks.12.mlp.fc1.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.out_blocks.12.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.12.mlp.fc2.weight sparsity: 0.29%
Layer base_model.model.out_blocks.12.mlp.fc2.bias sparsity: 0.13%
Layer base_model.model.out_blocks.12.mlp.fc2.lora_A.default.weight sparsity: 0.77%
Layer base_model.model.out_blocks.12.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.12.skip_linear.weight sparsity: 0.21%
Layer base_model.model.out_blocks.12.skip_linear.bias sparsity: 0.26%
Layer base_model.model.out_blocks.13.norm1.weight sparsity: 0.00%
Layer base_model.model.out_blocks.13.norm1.bias sparsity: 0.07%
Layer base_model.model.out_blocks.13.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.13.norm2.bias sparsity: 0.20%
Layer base_model.model.out_blocks.13.attn.qkv.weight sparsity: 0.20%
Layer base_model.model.out_blocks.13.attn.qkv.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.13.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.13.attn.proj.weight sparsity: 0.27%
Layer base_model.model.out_blocks.13.attn.proj.bias sparsity: 0.65%
Layer base_model.model.out_blocks.13.attn.proj.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.out_blocks.13.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.13.norm3.weight sparsity: 0.00%
Layer base_model.model.out_blocks.13.norm3.bias sparsity: 0.52%
Layer base_model.model.out_blocks.13.mlp.fc1.weight sparsity: 0.25%
Layer base_model.model.out_blocks.13.mlp.fc1.bias sparsity: 0.02%
Layer base_model.model.out_blocks.13.mlp.fc1.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.out_blocks.13.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.13.mlp.fc2.weight sparsity: 0.37%
Layer base_model.model.out_blocks.13.mlp.fc2.bias sparsity: 0.33%
Layer base_model.model.out_blocks.13.mlp.fc2.lora_A.default.weight sparsity: 0.79%
Layer base_model.model.out_blocks.13.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.13.skip_linear.weight sparsity: 0.21%
Layer base_model.model.out_blocks.13.skip_linear.bias sparsity: 0.20%
Layer base_model.model.out_blocks.14.norm1.weight sparsity: 0.20%
Layer base_model.model.out_blocks.14.norm1.bias sparsity: 0.33%
Layer base_model.model.out_blocks.14.norm2.weight sparsity: 0.00%
Layer base_model.model.out_blocks.14.norm2.bias sparsity: 0.78%
Layer base_model.model.out_blocks.14.attn.qkv.weight sparsity: 0.28%
Layer base_model.model.out_blocks.14.attn.qkv.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.out_blocks.14.attn.qkv.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.14.attn.proj.weight sparsity: 0.41%
Layer base_model.model.out_blocks.14.attn.proj.bias sparsity: 0.46%
Layer base_model.model.out_blocks.14.attn.proj.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.out_blocks.14.attn.proj.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.14.norm3.weight sparsity: 2.54%
Layer base_model.model.out_blocks.14.norm3.bias sparsity: 7.03%
Layer base_model.model.out_blocks.14.mlp.fc1.weight sparsity: 3.41%
Layer base_model.model.out_blocks.14.mlp.fc1.bias sparsity: 2.38%
Layer base_model.model.out_blocks.14.mlp.fc1.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.out_blocks.14.mlp.fc1.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.14.mlp.fc2.weight sparsity: 1.36%
Layer base_model.model.out_blocks.14.mlp.fc2.bias sparsity: 1.17%
Layer base_model.model.out_blocks.14.mlp.fc2.lora_A.default.weight sparsity: 0.80%
Layer base_model.model.out_blocks.14.mlp.fc2.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.out_blocks.14.skip_linear.weight sparsity: 0.24%
Layer base_model.model.out_blocks.14.skip_linear.bias sparsity: 0.39%
Layer base_model.model.norm.weight sparsity: 3.97%
Layer base_model.model.norm.bias sparsity: 8.66%
Layer base_model.model.decoder_pred.weight sparsity: 5.39%
Layer base_model.model.decoder_pred.bias sparsity: 6.25%
Layer base_model.model.adapters_itot.0.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.0.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.0.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.0.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.0.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_v.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.0.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.0.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.0.to_out.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.0.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.0.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_q.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.1.to_q.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_itot.1.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.1.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.1.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.1.to_v.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.1.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.1.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.1.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.1.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.2.to_q.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.adapters_itot.2.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.2.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.2.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_v.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.2.to_v.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.2.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.2.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.2.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.2.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.3.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.3.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.3.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.3.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_v.bias sparsity: 0.72%
Layer base_model.model.adapters_itot.3.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.3.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.3.to_out.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.3.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.3.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.4.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.4.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_k.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.4.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.4.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.4.to_v.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.4.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.4.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.4.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.4.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_q.bias sparsity: 0.13%
Layer base_model.model.adapters_itot.5.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.5.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_k.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.5.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.5.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.5.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.5.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.5.to_out.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.5.to_out.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.5.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_q.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.6.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.6.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.6.to_k.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.adapters_itot.6.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.6.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.6.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.6.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.6.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.6.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.7.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.7.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_k.bias sparsity: 0.72%
Layer base_model.model.adapters_itot.7.to_k.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.7.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.7.to_v.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.7.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.7.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.7.to_out.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.7.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_q.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.8.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.8.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.8.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.8.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_v.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.8.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.8.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.8.to_out.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.8.to_out.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.8.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_q.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.9.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.9.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.9.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.9.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.9.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.9.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.9.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.9.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.9.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.10.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.10.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_k.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.10.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.10.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.10.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.10.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.10.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.10.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.10.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.11.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.11.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.11.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.11.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.11.to_v.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.11.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.11.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.11.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.11.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.12.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.12.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.12.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.12.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.12.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.12.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.12.to_out.bias sparsity: 0.07%
Layer base_model.model.adapters_itot.12.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.12.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_q.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.13.to_q.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_itot.13.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.13.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.13.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.13.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.13.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.13.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.13.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.13.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.14.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.14.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.14.to_k.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.14.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.14.to_v.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.14.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.14.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.14.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.14.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_q.bias sparsity: 0.65%
Layer base_model.model.adapters_itot.15.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.15.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.15.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.15.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.15.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.15.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.15.to_out.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.15.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.15.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.16.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.16.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_k.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.16.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.16.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.16.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.16.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.16.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.16.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.16.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.17.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.17.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.17.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.17.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.17.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.17.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.17.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.17.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.17.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.18.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.18.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.18.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.18.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.18.to_v.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_itot.18.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.18.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.18.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.18.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_q.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.19.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.19.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_k.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.19.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.19.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.19.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.19.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.19.to_out.bias sparsity: 0.00%
Layer base_model.model.adapters_itot.19.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.19.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_q.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.20.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.20.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.20.to_k.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.20.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_v.bias sparsity: 0.13%
Layer base_model.model.adapters_itot.20.to_v.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.20.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.20.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.20.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.20.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.21.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.21.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.21.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.21.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.21.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.21.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.21.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.21.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.21.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_q.bias sparsity: 0.78%
Layer base_model.model.adapters_itot.22.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.22.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_k.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.22.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.22.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.22.to_v.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.22.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.22.to_out.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.22.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.22.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_q.bias sparsity: 0.72%
Layer base_model.model.adapters_itot.23.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.23.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.23.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.23.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.23.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.23.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.23.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.23.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_itot.23.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.24.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.24.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_k.bias sparsity: 0.07%
Layer base_model.model.adapters_itot.24.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.24.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_v.bias sparsity: 0.59%
Layer base_model.model.adapters_itot.24.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.24.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.24.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_itot.24.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.24.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_q.bias sparsity: 0.13%
Layer base_model.model.adapters_itot.25.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.25.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.25.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.25.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_itot.25.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.25.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.25.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.25.to_out.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.adapters_itot.25.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_q.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.26.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.26.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_k.bias sparsity: 0.65%
Layer base_model.model.adapters_itot.26.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.26.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.26.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.26.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.26.to_out.bias sparsity: 0.65%
Layer base_model.model.adapters_itot.26.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.26.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_q.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.27.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.27.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.27.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.27.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.27.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.27.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.27.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.27.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.27.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.28.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_itot.28.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_k.bias sparsity: 0.13%
Layer base_model.model.adapters_itot.28.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.28.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.28.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.28.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.28.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.28.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_itot.28.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_itot.29.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_itot.29.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_itot.29.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.29.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_itot.29.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.29.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_itot.29.to_out.bias sparsity: 0.52%
Layer base_model.model.adapters_itot.29.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_itot.29.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.0.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.0.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_k.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.0.to_k.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.adapters_ttoi.0.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.0.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.0.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.0.to_out.bias sparsity: 0.13%
Layer base_model.model.adapters_ttoi.0.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.0.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_q.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.1.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.1.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.1.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.1.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.1.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.1.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.1.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.1.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.1.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.2.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.2.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_k.bias sparsity: 0.78%
Layer base_model.model.adapters_ttoi.2.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.2.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_v.bias sparsity: 0.72%
Layer base_model.model.adapters_ttoi.2.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.2.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.2.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.2.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.2.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_q.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.3.to_q.lora_A.default.weight sparsity: 0.43%
Layer base_model.model.adapters_ttoi.3.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.3.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.3.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.3.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.3.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.3.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.3.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.3.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.4.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.4.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.4.to_k.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.4.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.4.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.4.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.4.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.4.to_out.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.adapters_ttoi.4.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.5.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.5.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.5.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.5.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_v.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.5.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.5.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.5.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.5.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.5.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_q.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.6.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.6.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.6.to_k.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.6.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_v.bias sparsity: 0.13%
Layer base_model.model.adapters_ttoi.6.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.6.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.6.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.6.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.6.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_q.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.7.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.7.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_k.bias sparsity: 0.72%
Layer base_model.model.adapters_ttoi.7.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.7.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_v.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.7.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.7.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.7.to_out.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.7.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.7.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.8.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.8.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.8.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.8.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_v.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.8.to_v.lora_A.default.weight sparsity: 0.43%
Layer base_model.model.adapters_ttoi.8.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.8.to_out.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.8.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.8.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_q.bias sparsity: 0.72%
Layer base_model.model.adapters_ttoi.9.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.9.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_k.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.9.to_k.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.9.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.9.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.9.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.9.to_out.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.9.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.9.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_q.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.10.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.10.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.10.to_k.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.10.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.10.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.10.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.10.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.10.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.10.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.11.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.11.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.11.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.11.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.11.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.11.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.11.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.11.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.11.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.12.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.12.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.12.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.12.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.12.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.12.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.12.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.12.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.12.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_q.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.13.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.13.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_k.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.13.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.13.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.13.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.13.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.13.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.13.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.13.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.14.to_q.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.adapters_ttoi.14.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.14.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.14.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_v.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.14.to_v.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.14.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.14.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.14.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.14.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_q.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.15.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.15.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.15.to_k.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_ttoi.15.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.15.to_v.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.15.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.15.to_out.bias sparsity: 0.13%
Layer base_model.model.adapters_ttoi.15.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.15.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_q.bias sparsity: 0.72%
Layer base_model.model.adapters_ttoi.16.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.16.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.16.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.16.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_v.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.16.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.16.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.16.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.16.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.16.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_q.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.17.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.17.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_k.bias sparsity: 0.65%
Layer base_model.model.adapters_ttoi.17.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.17.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.17.to_v.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_ttoi.17.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.17.to_out.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.17.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.17.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.18.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.18.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.18.to_k.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.18.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.18.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.18.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.18.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.18.to_out.lora_A.default.weight sparsity: 0.42%
Layer base_model.model.adapters_ttoi.18.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_q.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.19.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.19.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.19.to_k.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.19.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_v.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.19.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.19.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.19.to_out.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.19.to_out.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.19.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_q.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.20.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.20.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_k.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.20.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.20.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_v.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.20.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.20.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.20.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.20.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.20.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.21.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.21.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.21.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.21.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.21.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.21.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.21.to_out.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.21.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.21.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.22.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.22.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.22.to_k.lora_A.default.weight sparsity: 0.36%
Layer base_model.model.adapters_ttoi.22.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.22.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.22.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.22.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.22.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.22.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.23.to_q.lora_A.default.weight sparsity: 0.41%
Layer base_model.model.adapters_ttoi.23.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_k.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.23.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.23.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_v.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.23.to_v.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.23.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.23.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.23.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.23.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_q.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.24.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.24.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_k.bias sparsity: 0.07%
Layer base_model.model.adapters_ttoi.24.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.24.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.24.to_v.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.24.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.24.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.24.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.24.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_q.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.25.to_q.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.25.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_k.bias sparsity: 0.13%
Layer base_model.model.adapters_ttoi.25.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.25.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_v.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.25.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.25.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.25.to_out.bias sparsity: 0.13%
Layer base_model.model.adapters_ttoi.25.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.25.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_q.bias sparsity: 0.33%
Layer base_model.model.adapters_ttoi.26.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.26.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_k.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.26.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.26.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.26.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.26.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.26.to_out.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.26.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.26.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_q.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.27.to_q.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.27.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.27.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.27.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_v.bias sparsity: 0.52%
Layer base_model.model.adapters_ttoi.27.to_v.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.27.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.27.to_out.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.27.to_out.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.27.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_q.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.28.to_q.lora_A.default.weight sparsity: 0.37%
Layer base_model.model.adapters_ttoi.28.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_k.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.28.to_k.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.28.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_v.bias sparsity: 0.20%
Layer base_model.model.adapters_ttoi.28.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.28.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.28.to_out.bias sparsity: 0.13%
Layer base_model.model.adapters_ttoi.28.to_out.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.28.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_q.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_q.bias sparsity: 0.59%
Layer base_model.model.adapters_ttoi.29.to_q.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.29.to_q.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_k.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_k.bias sparsity: 0.26%
Layer base_model.model.adapters_ttoi.29.to_k.lora_A.default.weight sparsity: 0.40%
Layer base_model.model.adapters_ttoi.29.to_k.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_v.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_v.bias sparsity: 0.46%
Layer base_model.model.adapters_ttoi.29.to_v.lora_A.default.weight sparsity: 0.39%
Layer base_model.model.adapters_ttoi.29.to_v.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_out.weight sparsity: 100.00%
Layer base_model.model.adapters_ttoi.29.to_out.bias sparsity: 0.39%
Layer base_model.model.adapters_ttoi.29.to_out.lora_A.default.weight sparsity: 0.38%
Layer base_model.model.adapters_ttoi.29.to_out.lora_B.default.weight sparsity: 100.00%
Layer base_model.model.token_embedding.weight sparsity: 0.00%
Total sparsity: 39.26%
lora 192555008
Total parameters: 1711664208
Layer base_model.model.patch_embed.proj.lora_A.default.weight (torch.Size([128, 4, 2, 2])):
 - Same sparsity positions: 2047
 - Sparsity_Different parameters: 1
 - Different parameters: 2048
Layer base_model.model.patch_embed.proj.lora_B.default.weight (torch.Size([1536, 128, 1, 1])):
 - Same sparsity positions: 103326
 - Sparsity_Different parameters: 93282
 - Different parameters: 196608
Layer base_model.model.text_embed.lora_A.default.weight (torch.Size([128, 64])):
 - Same sparsity positions: 8177
 - Sparsity_Different parameters: 15
 - Different parameters: 8192
Layer base_model.model.text_embed.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 36518
 - Sparsity_Different parameters: 160090
 - Different parameters: 196608
Layer base_model.model.clip_img_embed.lora_A.default.weight (torch.Size([128, 512])):
 - Same sparsity positions: 65208
 - Sparsity_Different parameters: 328
 - Different parameters: 65536
Layer base_model.model.clip_img_embed.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 30633
 - Sparsity_Different parameters: 165975
 - Different parameters: 196608
Layer base_model.model.in_blocks.0.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195050
 - Sparsity_Different parameters: 1558
 - Different parameters: 196608
Layer base_model.model.in_blocks.0.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 112960
 - Sparsity_Different parameters: 476864
 - Different parameters: 589824
Layer base_model.model.in_blocks.0.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195092
 - Sparsity_Different parameters: 1516
 - Different parameters: 196608
Layer base_model.model.in_blocks.0.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 41099
 - Sparsity_Different parameters: 155509
 - Different parameters: 196608
Layer base_model.model.in_blocks.0.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195122
 - Sparsity_Different parameters: 1486
 - Different parameters: 196608
Layer base_model.model.in_blocks.0.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 131457
 - Sparsity_Different parameters: 654975
 - Different parameters: 786432
Layer base_model.model.in_blocks.0.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774292
 - Sparsity_Different parameters: 12140
 - Different parameters: 786432
Layer base_model.model.in_blocks.0.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 33111
 - Sparsity_Different parameters: 163497
 - Different parameters: 196608
Layer base_model.model.in_blocks.1.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195086
 - Sparsity_Different parameters: 1522
 - Different parameters: 196608
Layer base_model.model.in_blocks.1.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 98968
 - Sparsity_Different parameters: 490856
 - Different parameters: 589824
Layer base_model.model.in_blocks.1.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195056
 - Sparsity_Different parameters: 1552
 - Different parameters: 196608
Layer base_model.model.in_blocks.1.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 33366
 - Sparsity_Different parameters: 163242
 - Different parameters: 196608
Layer base_model.model.in_blocks.1.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195105
 - Sparsity_Different parameters: 1503
 - Different parameters: 196608
Layer base_model.model.in_blocks.1.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 151056
 - Sparsity_Different parameters: 635376
 - Different parameters: 786432
Layer base_model.model.in_blocks.1.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774029
 - Sparsity_Different parameters: 12403
 - Different parameters: 786432
Layer base_model.model.in_blocks.1.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 30939
 - Sparsity_Different parameters: 165669
 - Different parameters: 196608
Layer base_model.model.in_blocks.2.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195059
 - Sparsity_Different parameters: 1549
 - Different parameters: 196608
Layer base_model.model.in_blocks.2.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 101995
 - Sparsity_Different parameters: 487829
 - Different parameters: 589824
Layer base_model.model.in_blocks.2.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195141
 - Sparsity_Different parameters: 1467
 - Different parameters: 196608
Layer base_model.model.in_blocks.2.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 31815
 - Sparsity_Different parameters: 164793
 - Different parameters: 196608
Layer base_model.model.in_blocks.2.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195112
 - Sparsity_Different parameters: 1496
 - Different parameters: 196608
Layer base_model.model.in_blocks.2.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 137834
 - Sparsity_Different parameters: 648598
 - Different parameters: 786432
Layer base_model.model.in_blocks.2.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774272
 - Sparsity_Different parameters: 12160
 - Different parameters: 786432
Layer base_model.model.in_blocks.2.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 30424
 - Sparsity_Different parameters: 166184
 - Different parameters: 196608
Layer base_model.model.in_blocks.3.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195030
 - Sparsity_Different parameters: 1578
 - Different parameters: 196608
Layer base_model.model.in_blocks.3.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 108137
 - Sparsity_Different parameters: 481687
 - Different parameters: 589824
Layer base_model.model.in_blocks.3.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195088
 - Sparsity_Different parameters: 1520
 - Different parameters: 196608
Layer base_model.model.in_blocks.3.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 31711
 - Sparsity_Different parameters: 164897
 - Different parameters: 196608
Layer base_model.model.in_blocks.3.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195041
 - Sparsity_Different parameters: 1567
 - Different parameters: 196608
Layer base_model.model.in_blocks.3.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 132496
 - Sparsity_Different parameters: 653936
 - Different parameters: 786432
Layer base_model.model.in_blocks.3.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774042
 - Sparsity_Different parameters: 12390
 - Different parameters: 786432
Layer base_model.model.in_blocks.3.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 29995
 - Sparsity_Different parameters: 166613
 - Different parameters: 196608
Layer base_model.model.in_blocks.4.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195041
 - Sparsity_Different parameters: 1567
 - Different parameters: 196608
Layer base_model.model.in_blocks.4.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 102234
 - Sparsity_Different parameters: 487590
 - Different parameters: 589824
Layer base_model.model.in_blocks.4.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195070
 - Sparsity_Different parameters: 1538
 - Different parameters: 196608
Layer base_model.model.in_blocks.4.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 32832
 - Sparsity_Different parameters: 163776
 - Different parameters: 196608
Layer base_model.model.in_blocks.4.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195177
 - Sparsity_Different parameters: 1431
 - Different parameters: 196608
Layer base_model.model.in_blocks.4.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 131064
 - Sparsity_Different parameters: 655368
 - Different parameters: 786432
Layer base_model.model.in_blocks.4.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774142
 - Sparsity_Different parameters: 12290
 - Different parameters: 786432
Layer base_model.model.in_blocks.4.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 29804
 - Sparsity_Different parameters: 166804
 - Different parameters: 196608
Layer base_model.model.in_blocks.5.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195079
 - Sparsity_Different parameters: 1529
 - Different parameters: 196608
Layer base_model.model.in_blocks.5.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 100261
 - Sparsity_Different parameters: 489563
 - Different parameters: 589824
Layer base_model.model.in_blocks.5.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195121
 - Sparsity_Different parameters: 1487
 - Different parameters: 196608
Layer base_model.model.in_blocks.5.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 33863
 - Sparsity_Different parameters: 162745
 - Different parameters: 196608
Layer base_model.model.in_blocks.5.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195053
 - Sparsity_Different parameters: 1555
 - Different parameters: 196608
Layer base_model.model.in_blocks.5.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 131674
 - Sparsity_Different parameters: 654758
 - Different parameters: 786432
Layer base_model.model.in_blocks.5.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774118
 - Sparsity_Different parameters: 12314
 - Different parameters: 786432
Layer base_model.model.in_blocks.5.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 30317
 - Sparsity_Different parameters: 166291
 - Different parameters: 196608
Layer base_model.model.in_blocks.6.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195049
 - Sparsity_Different parameters: 1559
 - Different parameters: 196608
Layer base_model.model.in_blocks.6.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 106010
 - Sparsity_Different parameters: 483814
 - Different parameters: 589824
Layer base_model.model.in_blocks.6.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195081
 - Sparsity_Different parameters: 1527
 - Different parameters: 196608
Layer base_model.model.in_blocks.6.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 35822
 - Sparsity_Different parameters: 160786
 - Different parameters: 196608
Layer base_model.model.in_blocks.6.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195077
 - Sparsity_Different parameters: 1531
 - Different parameters: 196608
Layer base_model.model.in_blocks.6.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 135887
 - Sparsity_Different parameters: 650545
 - Different parameters: 786432
Layer base_model.model.in_blocks.6.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 773933
 - Sparsity_Different parameters: 12499
 - Different parameters: 786432
Layer base_model.model.in_blocks.6.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 30510
 - Sparsity_Different parameters: 166098
 - Different parameters: 196608
Layer base_model.model.in_blocks.7.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195079
 - Sparsity_Different parameters: 1529
 - Different parameters: 196608
Layer base_model.model.in_blocks.7.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 101539
 - Sparsity_Different parameters: 488285
 - Different parameters: 589824
Layer base_model.model.in_blocks.7.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195068
 - Sparsity_Different parameters: 1540
 - Different parameters: 196608
Layer base_model.model.in_blocks.7.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 32826
 - Sparsity_Different parameters: 163782
 - Different parameters: 196608
Layer base_model.model.in_blocks.7.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 194974
 - Sparsity_Different parameters: 1634
 - Different parameters: 196608
Layer base_model.model.in_blocks.7.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 128818
 - Sparsity_Different parameters: 657614
 - Different parameters: 786432
Layer base_model.model.in_blocks.7.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774242
 - Sparsity_Different parameters: 12190
 - Different parameters: 786432
Layer base_model.model.in_blocks.7.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 30696
 - Sparsity_Different parameters: 165912
 - Different parameters: 196608
Layer base_model.model.in_blocks.8.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195110
 - Sparsity_Different parameters: 1498
 - Different parameters: 196608
Layer base_model.model.in_blocks.8.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 95956
 - Sparsity_Different parameters: 493868
 - Different parameters: 589824
Layer base_model.model.in_blocks.8.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195065
 - Sparsity_Different parameters: 1543
 - Different parameters: 196608
Layer base_model.model.in_blocks.8.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 31234
 - Sparsity_Different parameters: 165374
 - Different parameters: 196608
Layer base_model.model.in_blocks.8.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195076
 - Sparsity_Different parameters: 1532
 - Different parameters: 196608
Layer base_model.model.in_blocks.8.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 124080
 - Sparsity_Different parameters: 662352
 - Different parameters: 786432
Layer base_model.model.in_blocks.8.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774247
 - Sparsity_Different parameters: 12185
 - Different parameters: 786432
Layer base_model.model.in_blocks.8.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 29167
 - Sparsity_Different parameters: 167441
 - Different parameters: 196608
Layer base_model.model.in_blocks.9.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195123
 - Sparsity_Different parameters: 1485
 - Different parameters: 196608
Layer base_model.model.in_blocks.9.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 91977
 - Sparsity_Different parameters: 497847
 - Different parameters: 589824
Layer base_model.model.in_blocks.9.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195024
 - Sparsity_Different parameters: 1584
 - Different parameters: 196608
Layer base_model.model.in_blocks.9.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 30736
 - Sparsity_Different parameters: 165872
 - Different parameters: 196608
Layer base_model.model.in_blocks.9.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195066
 - Sparsity_Different parameters: 1542
 - Different parameters: 196608
Layer base_model.model.in_blocks.9.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 123157
 - Sparsity_Different parameters: 663275
 - Different parameters: 786432
Layer base_model.model.in_blocks.9.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774287
 - Sparsity_Different parameters: 12145
 - Different parameters: 786432
Layer base_model.model.in_blocks.9.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 26944
 - Sparsity_Different parameters: 169664
 - Different parameters: 196608
Layer base_model.model.in_blocks.10.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195117
 - Sparsity_Different parameters: 1491
 - Different parameters: 196608
Layer base_model.model.in_blocks.10.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 89879
 - Sparsity_Different parameters: 499945
 - Different parameters: 589824
Layer base_model.model.in_blocks.10.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195013
 - Sparsity_Different parameters: 1595
 - Different parameters: 196608
Layer base_model.model.in_blocks.10.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 28788
 - Sparsity_Different parameters: 167820
 - Different parameters: 196608
Layer base_model.model.in_blocks.10.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195088
 - Sparsity_Different parameters: 1520
 - Different parameters: 196608
Layer base_model.model.in_blocks.10.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 117595
 - Sparsity_Different parameters: 668837
 - Different parameters: 786432
Layer base_model.model.in_blocks.10.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774215
 - Sparsity_Different parameters: 12217
 - Different parameters: 786432
Layer base_model.model.in_blocks.10.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 29464
 - Sparsity_Different parameters: 167144
 - Different parameters: 196608
Layer base_model.model.in_blocks.11.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195148
 - Sparsity_Different parameters: 1460
 - Different parameters: 196608
Layer base_model.model.in_blocks.11.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 92355
 - Sparsity_Different parameters: 497469
 - Different parameters: 589824
Layer base_model.model.in_blocks.11.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195062
 - Sparsity_Different parameters: 1546
 - Different parameters: 196608
Layer base_model.model.in_blocks.11.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 29091
 - Sparsity_Different parameters: 167517
 - Different parameters: 196608
Layer base_model.model.in_blocks.11.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195096
 - Sparsity_Different parameters: 1512
 - Different parameters: 196608
Layer base_model.model.in_blocks.11.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 118223
 - Sparsity_Different parameters: 668209
 - Different parameters: 786432
Layer base_model.model.in_blocks.11.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774172
 - Sparsity_Different parameters: 12260
 - Different parameters: 786432
Layer base_model.model.in_blocks.11.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 29280
 - Sparsity_Different parameters: 167328
 - Different parameters: 196608
Layer base_model.model.in_blocks.12.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 194974
 - Sparsity_Different parameters: 1634
 - Different parameters: 196608
Layer base_model.model.in_blocks.12.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 89753
 - Sparsity_Different parameters: 500071
 - Different parameters: 589824
Layer base_model.model.in_blocks.12.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195075
 - Sparsity_Different parameters: 1533
 - Different parameters: 196608
Layer base_model.model.in_blocks.12.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 27711
 - Sparsity_Different parameters: 168897
 - Different parameters: 196608
Layer base_model.model.in_blocks.12.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195047
 - Sparsity_Different parameters: 1561
 - Different parameters: 196608
Layer base_model.model.in_blocks.12.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 107328
 - Sparsity_Different parameters: 679104
 - Different parameters: 786432
Layer base_model.model.in_blocks.12.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774216
 - Sparsity_Different parameters: 12216
 - Different parameters: 786432
Layer base_model.model.in_blocks.12.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 24518
 - Sparsity_Different parameters: 172090
 - Different parameters: 196608
Layer base_model.model.in_blocks.13.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195036
 - Sparsity_Different parameters: 1572
 - Different parameters: 196608
Layer base_model.model.in_blocks.13.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 90458
 - Sparsity_Different parameters: 499366
 - Different parameters: 589824
Layer base_model.model.in_blocks.13.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195060
 - Sparsity_Different parameters: 1548
 - Different parameters: 196608
Layer base_model.model.in_blocks.13.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 26126
 - Sparsity_Different parameters: 170482
 - Different parameters: 196608
Layer base_model.model.in_blocks.13.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195146
 - Sparsity_Different parameters: 1462
 - Different parameters: 196608
Layer base_model.model.in_blocks.13.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 101581
 - Sparsity_Different parameters: 684851
 - Different parameters: 786432
Layer base_model.model.in_blocks.13.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774132
 - Sparsity_Different parameters: 12300
 - Different parameters: 786432
Layer base_model.model.in_blocks.13.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 22750
 - Sparsity_Different parameters: 173858
 - Different parameters: 196608
Layer base_model.model.in_blocks.14.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195057
 - Sparsity_Different parameters: 1551
 - Different parameters: 196608
Layer base_model.model.in_blocks.14.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 87052
 - Sparsity_Different parameters: 502772
 - Different parameters: 589824
Layer base_model.model.in_blocks.14.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195126
 - Sparsity_Different parameters: 1482
 - Different parameters: 196608
Layer base_model.model.in_blocks.14.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 26200
 - Sparsity_Different parameters: 170408
 - Different parameters: 196608
Layer base_model.model.in_blocks.14.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195096
 - Sparsity_Different parameters: 1512
 - Different parameters: 196608
Layer base_model.model.in_blocks.14.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 108376
 - Sparsity_Different parameters: 678056
 - Different parameters: 786432
Layer base_model.model.in_blocks.14.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774318
 - Sparsity_Different parameters: 12114
 - Different parameters: 786432
Layer base_model.model.in_blocks.14.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 25768
 - Sparsity_Different parameters: 170840
 - Different parameters: 196608
Layer base_model.model.mid_block.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195069
 - Sparsity_Different parameters: 1539
 - Different parameters: 196608
Layer base_model.model.mid_block.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 92534
 - Sparsity_Different parameters: 497290
 - Different parameters: 589824
Layer base_model.model.mid_block.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195088
 - Sparsity_Different parameters: 1520
 - Different parameters: 196608
Layer base_model.model.mid_block.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 28326
 - Sparsity_Different parameters: 168282
 - Different parameters: 196608
Layer base_model.model.mid_block.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195053
 - Sparsity_Different parameters: 1555
 - Different parameters: 196608
Layer base_model.model.mid_block.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 107952
 - Sparsity_Different parameters: 678480
 - Different parameters: 786432
Layer base_model.model.mid_block.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774289
 - Sparsity_Different parameters: 12143
 - Different parameters: 786432
Layer base_model.model.mid_block.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 26400
 - Sparsity_Different parameters: 170208
 - Different parameters: 196608
Layer base_model.model.out_blocks.0.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195113
 - Sparsity_Different parameters: 1495
 - Different parameters: 196608
Layer base_model.model.out_blocks.0.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 86795
 - Sparsity_Different parameters: 503029
 - Different parameters: 589824
Layer base_model.model.out_blocks.0.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195080
 - Sparsity_Different parameters: 1528
 - Different parameters: 196608
Layer base_model.model.out_blocks.0.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 25041
 - Sparsity_Different parameters: 171567
 - Different parameters: 196608
Layer base_model.model.out_blocks.0.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195035
 - Sparsity_Different parameters: 1573
 - Different parameters: 196608
Layer base_model.model.out_blocks.0.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 97684
 - Sparsity_Different parameters: 688748
 - Different parameters: 786432
Layer base_model.model.out_blocks.0.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774260
 - Sparsity_Different parameters: 12172
 - Different parameters: 786432
Layer base_model.model.out_blocks.0.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 23544
 - Sparsity_Different parameters: 173064
 - Different parameters: 196608
Layer base_model.model.out_blocks.1.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195123
 - Sparsity_Different parameters: 1485
 - Different parameters: 196608
Layer base_model.model.out_blocks.1.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 72101
 - Sparsity_Different parameters: 517723
 - Different parameters: 589824
Layer base_model.model.out_blocks.1.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195144
 - Sparsity_Different parameters: 1464
 - Different parameters: 196608
Layer base_model.model.out_blocks.1.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 22019
 - Sparsity_Different parameters: 174589
 - Different parameters: 196608
Layer base_model.model.out_blocks.1.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195061
 - Sparsity_Different parameters: 1547
 - Different parameters: 196608
Layer base_model.model.out_blocks.1.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 83486
 - Sparsity_Different parameters: 702946
 - Different parameters: 786432
Layer base_model.model.out_blocks.1.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774191
 - Sparsity_Different parameters: 12241
 - Different parameters: 786432
Layer base_model.model.out_blocks.1.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 24397
 - Sparsity_Different parameters: 172211
 - Different parameters: 196608
Layer base_model.model.out_blocks.2.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195124
 - Sparsity_Different parameters: 1484
 - Different parameters: 196608
Layer base_model.model.out_blocks.2.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 70780
 - Sparsity_Different parameters: 519044
 - Different parameters: 589824
Layer base_model.model.out_blocks.2.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195059
 - Sparsity_Different parameters: 1549
 - Different parameters: 196608
Layer base_model.model.out_blocks.2.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 22037
 - Sparsity_Different parameters: 174571
 - Different parameters: 196608
Layer base_model.model.out_blocks.2.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 194977
 - Sparsity_Different parameters: 1631
 - Different parameters: 196608
Layer base_model.model.out_blocks.2.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 82991
 - Sparsity_Different parameters: 703441
 - Different parameters: 786432
Layer base_model.model.out_blocks.2.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774226
 - Sparsity_Different parameters: 12206
 - Different parameters: 786432
Layer base_model.model.out_blocks.2.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 25511
 - Sparsity_Different parameters: 171097
 - Different parameters: 196608
Layer base_model.model.out_blocks.3.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195079
 - Sparsity_Different parameters: 1529
 - Different parameters: 196608
Layer base_model.model.out_blocks.3.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 79049
 - Sparsity_Different parameters: 510775
 - Different parameters: 589824
Layer base_model.model.out_blocks.3.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195082
 - Sparsity_Different parameters: 1526
 - Different parameters: 196608
Layer base_model.model.out_blocks.3.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 23317
 - Sparsity_Different parameters: 173291
 - Different parameters: 196608
Layer base_model.model.out_blocks.3.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 194986
 - Sparsity_Different parameters: 1622
 - Different parameters: 196608
Layer base_model.model.out_blocks.3.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 104728
 - Sparsity_Different parameters: 681704
 - Different parameters: 786432
Layer base_model.model.out_blocks.3.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774180
 - Sparsity_Different parameters: 12252
 - Different parameters: 786431
Layer base_model.model.out_blocks.3.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 27504
 - Sparsity_Different parameters: 169104
 - Different parameters: 196608
Layer base_model.model.out_blocks.4.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195080
 - Sparsity_Different parameters: 1528
 - Different parameters: 196608
Layer base_model.model.out_blocks.4.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 81898
 - Sparsity_Different parameters: 507926
 - Different parameters: 589824
Layer base_model.model.out_blocks.4.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195054
 - Sparsity_Different parameters: 1554
 - Different parameters: 196608
Layer base_model.model.out_blocks.4.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 24205
 - Sparsity_Different parameters: 172403
 - Different parameters: 196608
Layer base_model.model.out_blocks.4.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195084
 - Sparsity_Different parameters: 1524
 - Different parameters: 196608
Layer base_model.model.out_blocks.4.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 110020
 - Sparsity_Different parameters: 676412
 - Different parameters: 786432
Layer base_model.model.out_blocks.4.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774211
 - Sparsity_Different parameters: 12221
 - Different parameters: 786432
Layer base_model.model.out_blocks.4.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 30610
 - Sparsity_Different parameters: 165998
 - Different parameters: 196608
Layer base_model.model.out_blocks.5.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195082
 - Sparsity_Different parameters: 1526
 - Different parameters: 196608
Layer base_model.model.out_blocks.5.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 86855
 - Sparsity_Different parameters: 502969
 - Different parameters: 589824
Layer base_model.model.out_blocks.5.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195071
 - Sparsity_Different parameters: 1537
 - Different parameters: 196608
Layer base_model.model.out_blocks.5.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 25905
 - Sparsity_Different parameters: 170703
 - Different parameters: 196608
Layer base_model.model.out_blocks.5.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195048
 - Sparsity_Different parameters: 1560
 - Different parameters: 196608
Layer base_model.model.out_blocks.5.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 114380
 - Sparsity_Different parameters: 672052
 - Different parameters: 786432
Layer base_model.model.out_blocks.5.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774318
 - Sparsity_Different parameters: 12114
 - Different parameters: 786432
Layer base_model.model.out_blocks.5.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 29056
 - Sparsity_Different parameters: 167552
 - Different parameters: 196608
Layer base_model.model.out_blocks.6.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195057
 - Sparsity_Different parameters: 1551
 - Different parameters: 196608
Layer base_model.model.out_blocks.6.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 86930
 - Sparsity_Different parameters: 502894
 - Different parameters: 589824
Layer base_model.model.out_blocks.6.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195022
 - Sparsity_Different parameters: 1586
 - Different parameters: 196608
Layer base_model.model.out_blocks.6.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 27865
 - Sparsity_Different parameters: 168743
 - Different parameters: 196608
Layer base_model.model.out_blocks.6.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195104
 - Sparsity_Different parameters: 1504
 - Different parameters: 196608
Layer base_model.model.out_blocks.6.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 122209
 - Sparsity_Different parameters: 664223
 - Different parameters: 786432
Layer base_model.model.out_blocks.6.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774223
 - Sparsity_Different parameters: 12209
 - Different parameters: 786432
Layer base_model.model.out_blocks.6.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 32766
 - Sparsity_Different parameters: 163842
 - Different parameters: 196608
Layer base_model.model.out_blocks.7.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 194993
 - Sparsity_Different parameters: 1615
 - Different parameters: 196608
Layer base_model.model.out_blocks.7.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 93916
 - Sparsity_Different parameters: 495908
 - Different parameters: 589824
Layer base_model.model.out_blocks.7.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195149
 - Sparsity_Different parameters: 1459
 - Different parameters: 196608
Layer base_model.model.out_blocks.7.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 30355
 - Sparsity_Different parameters: 166253
 - Different parameters: 196608
Layer base_model.model.out_blocks.7.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 194986
 - Sparsity_Different parameters: 1622
 - Different parameters: 196608
Layer base_model.model.out_blocks.7.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 127850
 - Sparsity_Different parameters: 658582
 - Different parameters: 786432
Layer base_model.model.out_blocks.7.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774151
 - Sparsity_Different parameters: 12281
 - Different parameters: 786432
Layer base_model.model.out_blocks.7.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 34655
 - Sparsity_Different parameters: 161953
 - Different parameters: 196608
Layer base_model.model.out_blocks.8.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 194998
 - Sparsity_Different parameters: 1610
 - Different parameters: 196608
Layer base_model.model.out_blocks.8.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 103132
 - Sparsity_Different parameters: 486692
 - Different parameters: 589824
Layer base_model.model.out_blocks.8.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195030
 - Sparsity_Different parameters: 1578
 - Different parameters: 196608
Layer base_model.model.out_blocks.8.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 33652
 - Sparsity_Different parameters: 162956
 - Different parameters: 196608
Layer base_model.model.out_blocks.8.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195059
 - Sparsity_Different parameters: 1549
 - Different parameters: 196608
Layer base_model.model.out_blocks.8.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 133594
 - Sparsity_Different parameters: 652838
 - Different parameters: 786432
Layer base_model.model.out_blocks.8.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774300
 - Sparsity_Different parameters: 12132
 - Different parameters: 786432
Layer base_model.model.out_blocks.8.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 36326
 - Sparsity_Different parameters: 160282
 - Different parameters: 196608
Layer base_model.model.out_blocks.9.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 194975
 - Sparsity_Different parameters: 1633
 - Different parameters: 196608
Layer base_model.model.out_blocks.9.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 100997
 - Sparsity_Different parameters: 488827
 - Different parameters: 589824
Layer base_model.model.out_blocks.9.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195056
 - Sparsity_Different parameters: 1552
 - Different parameters: 196608
Layer base_model.model.out_blocks.9.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 35732
 - Sparsity_Different parameters: 160876
 - Different parameters: 196608
Layer base_model.model.out_blocks.9.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195146
 - Sparsity_Different parameters: 1462
 - Different parameters: 196608
Layer base_model.model.out_blocks.9.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 126459
 - Sparsity_Different parameters: 659973
 - Different parameters: 786432
Layer base_model.model.out_blocks.9.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 773988
 - Sparsity_Different parameters: 12444
 - Different parameters: 786432
Layer base_model.model.out_blocks.9.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 33037
 - Sparsity_Different parameters: 163571
 - Different parameters: 196608
Layer base_model.model.out_blocks.10.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195086
 - Sparsity_Different parameters: 1522
 - Different parameters: 196608
Layer base_model.model.out_blocks.10.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 89296
 - Sparsity_Different parameters: 500528
 - Different parameters: 589824
Layer base_model.model.out_blocks.10.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195033
 - Sparsity_Different parameters: 1575
 - Different parameters: 196608
Layer base_model.model.out_blocks.10.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 32801
 - Sparsity_Different parameters: 163807
 - Different parameters: 196608
Layer base_model.model.out_blocks.10.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195009
 - Sparsity_Different parameters: 1599
 - Different parameters: 196608
Layer base_model.model.out_blocks.10.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 104954
 - Sparsity_Different parameters: 681478
 - Different parameters: 786432
Layer base_model.model.out_blocks.10.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774266
 - Sparsity_Different parameters: 12166
 - Different parameters: 786432
Layer base_model.model.out_blocks.10.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 28140
 - Sparsity_Different parameters: 168468
 - Different parameters: 196608
Layer base_model.model.out_blocks.11.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195074
 - Sparsity_Different parameters: 1534
 - Different parameters: 196608
Layer base_model.model.out_blocks.11.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 72517
 - Sparsity_Different parameters: 517307
 - Different parameters: 589824
Layer base_model.model.out_blocks.11.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195096
 - Sparsity_Different parameters: 1512
 - Different parameters: 196608
Layer base_model.model.out_blocks.11.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 27888
 - Sparsity_Different parameters: 168720
 - Different parameters: 196608
Layer base_model.model.out_blocks.11.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195011
 - Sparsity_Different parameters: 1597
 - Different parameters: 196608
Layer base_model.model.out_blocks.11.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 89759
 - Sparsity_Different parameters: 696673
 - Different parameters: 786432
Layer base_model.model.out_blocks.11.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774391
 - Sparsity_Different parameters: 12041
 - Different parameters: 786432
Layer base_model.model.out_blocks.11.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 29301
 - Sparsity_Different parameters: 167307
 - Different parameters: 196608
Layer base_model.model.out_blocks.12.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195017
 - Sparsity_Different parameters: 1591
 - Different parameters: 196608
Layer base_model.model.out_blocks.12.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 73323
 - Sparsity_Different parameters: 516501
 - Different parameters: 589824
Layer base_model.model.out_blocks.12.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195085
 - Sparsity_Different parameters: 1523
 - Different parameters: 196608
Layer base_model.model.out_blocks.12.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 24000
 - Sparsity_Different parameters: 172608
 - Different parameters: 196608
Layer base_model.model.out_blocks.12.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195067
 - Sparsity_Different parameters: 1541
 - Different parameters: 196608
Layer base_model.model.out_blocks.12.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 92553
 - Sparsity_Different parameters: 693879
 - Different parameters: 786432
Layer base_model.model.out_blocks.12.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774276
 - Sparsity_Different parameters: 12156
 - Different parameters: 786432
Layer base_model.model.out_blocks.12.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 30878
 - Sparsity_Different parameters: 165730
 - Different parameters: 196608
Layer base_model.model.out_blocks.13.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195070
 - Sparsity_Different parameters: 1538
 - Different parameters: 196608
Layer base_model.model.out_blocks.13.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 88060
 - Sparsity_Different parameters: 501764
 - Different parameters: 589824
Layer base_model.model.out_blocks.13.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195104
 - Sparsity_Different parameters: 1504
 - Different parameters: 196608
Layer base_model.model.out_blocks.13.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 28998
 - Sparsity_Different parameters: 167610
 - Different parameters: 196608
Layer base_model.model.out_blocks.13.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195073
 - Sparsity_Different parameters: 1535
 - Different parameters: 196608
Layer base_model.model.out_blocks.13.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 101436
 - Sparsity_Different parameters: 684996
 - Different parameters: 786432
Layer base_model.model.out_blocks.13.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 774129
 - Sparsity_Different parameters: 12303
 - Different parameters: 786432
Layer base_model.model.out_blocks.13.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 29678
 - Sparsity_Different parameters: 166930
 - Different parameters: 196608
Layer base_model.model.out_blocks.14.attn.qkv.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195118
 - Sparsity_Different parameters: 1490
 - Different parameters: 196608
Layer base_model.model.out_blocks.14.attn.qkv.lora_B.default.weight (torch.Size([4608, 128])):
 - Same sparsity positions: 86783
 - Sparsity_Different parameters: 503041
 - Different parameters: 589824
Layer base_model.model.out_blocks.14.attn.proj.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195070
 - Sparsity_Different parameters: 1538
 - Different parameters: 196608
Layer base_model.model.out_blocks.14.attn.proj.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 32450
 - Sparsity_Different parameters: 164158
 - Different parameters: 196608
Layer base_model.model.out_blocks.14.mlp.fc1.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195111
 - Sparsity_Different parameters: 1497
 - Different parameters: 196608
Layer base_model.model.out_blocks.14.mlp.fc1.lora_B.default.weight (torch.Size([6144, 128])):
 - Same sparsity positions: 137649
 - Sparsity_Different parameters: 648783
 - Different parameters: 786432
Layer base_model.model.out_blocks.14.mlp.fc2.lora_A.default.weight (torch.Size([128, 6144])):
 - Same sparsity positions: 773987
 - Sparsity_Different parameters: 12445
 - Different parameters: 786431
Layer base_model.model.out_blocks.14.mlp.fc2.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 34337
 - Sparsity_Different parameters: 162271
 - Different parameters: 196608
Layer base_model.model.adapters_itot.0.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195140
 - Sparsity_Different parameters: 1468
 - Different parameters: 196608
Layer base_model.model.adapters_itot.0.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.0.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195100
 - Sparsity_Different parameters: 1508
 - Different parameters: 196608
Layer base_model.model.adapters_itot.0.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.0.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195109
 - Sparsity_Different parameters: 1499
 - Different parameters: 196608
Layer base_model.model.adapters_itot.0.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 120239
 - Sparsity_Different parameters: 76369
 - Different parameters: 196608
Layer base_model.model.adapters_itot.0.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195079
 - Sparsity_Different parameters: 1529
 - Different parameters: 196608
Layer base_model.model.adapters_itot.0.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 39304
 - Sparsity_Different parameters: 157304
 - Different parameters: 196608
Layer base_model.model.adapters_itot.1.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195171
 - Sparsity_Different parameters: 1437
 - Different parameters: 196608
Layer base_model.model.adapters_itot.1.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.1.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195028
 - Sparsity_Different parameters: 1580
 - Different parameters: 196608
Layer base_model.model.adapters_itot.1.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.1.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195143
 - Sparsity_Different parameters: 1465
 - Different parameters: 196608
Layer base_model.model.adapters_itot.1.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 10014
 - Sparsity_Different parameters: 186594
 - Different parameters: 196608
Layer base_model.model.adapters_itot.1.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195031
 - Sparsity_Different parameters: 1577
 - Different parameters: 196608
Layer base_model.model.adapters_itot.1.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 23157
 - Sparsity_Different parameters: 173451
 - Different parameters: 196608
Layer base_model.model.adapters_itot.2.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195019
 - Sparsity_Different parameters: 1589
 - Different parameters: 196608
Layer base_model.model.adapters_itot.2.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.2.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195053
 - Sparsity_Different parameters: 1555
 - Different parameters: 196608
Layer base_model.model.adapters_itot.2.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.2.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195029
 - Sparsity_Different parameters: 1579
 - Different parameters: 196608
Layer base_model.model.adapters_itot.2.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 4929
 - Sparsity_Different parameters: 191679
 - Different parameters: 196608
Layer base_model.model.adapters_itot.2.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195053
 - Sparsity_Different parameters: 1555
 - Different parameters: 196608
Layer base_model.model.adapters_itot.2.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 21693
 - Sparsity_Different parameters: 174915
 - Different parameters: 196608
Layer base_model.model.adapters_itot.3.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195135
 - Sparsity_Different parameters: 1473
 - Different parameters: 196608
Layer base_model.model.adapters_itot.3.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.3.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195075
 - Sparsity_Different parameters: 1533
 - Different parameters: 196608
Layer base_model.model.adapters_itot.3.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.3.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195079
 - Sparsity_Different parameters: 1529
 - Different parameters: 196608
Layer base_model.model.adapters_itot.3.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 18810
 - Sparsity_Different parameters: 177798
 - Different parameters: 196608
Layer base_model.model.adapters_itot.3.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195154
 - Sparsity_Different parameters: 1454
 - Different parameters: 196608
Layer base_model.model.adapters_itot.3.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 27984
 - Sparsity_Different parameters: 168624
 - Different parameters: 196608
Layer base_model.model.adapters_itot.4.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195057
 - Sparsity_Different parameters: 1551
 - Different parameters: 196608
Layer base_model.model.adapters_itot.4.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.4.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195033
 - Sparsity_Different parameters: 1575
 - Different parameters: 196608
Layer base_model.model.adapters_itot.4.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.4.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195139
 - Sparsity_Different parameters: 1469
 - Different parameters: 196608
Layer base_model.model.adapters_itot.4.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 4717
 - Sparsity_Different parameters: 191891
 - Different parameters: 196608
Layer base_model.model.adapters_itot.4.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195109
 - Sparsity_Different parameters: 1499
 - Different parameters: 196608
Layer base_model.model.adapters_itot.4.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 20837
 - Sparsity_Different parameters: 175771
 - Different parameters: 196608
Layer base_model.model.adapters_itot.5.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195132
 - Sparsity_Different parameters: 1476
 - Different parameters: 196608
Layer base_model.model.adapters_itot.5.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.5.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195080
 - Sparsity_Different parameters: 1528
 - Different parameters: 196608
Layer base_model.model.adapters_itot.5.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.5.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 194995
 - Sparsity_Different parameters: 1613
 - Different parameters: 196608
Layer base_model.model.adapters_itot.5.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 2058
 - Sparsity_Different parameters: 194550
 - Different parameters: 196608
Layer base_model.model.adapters_itot.5.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195046
 - Sparsity_Different parameters: 1562
 - Different parameters: 196608
Layer base_model.model.adapters_itot.5.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 19387
 - Sparsity_Different parameters: 177221
 - Different parameters: 196608
Layer base_model.model.adapters_itot.6.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195046
 - Sparsity_Different parameters: 1562
 - Different parameters: 196608
Layer base_model.model.adapters_itot.6.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.6.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195075
 - Sparsity_Different parameters: 1533
 - Different parameters: 196608
Layer base_model.model.adapters_itot.6.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.6.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195059
 - Sparsity_Different parameters: 1549
 - Different parameters: 196608
Layer base_model.model.adapters_itot.6.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 1039
 - Sparsity_Different parameters: 195569
 - Different parameters: 196608
Layer base_model.model.adapters_itot.6.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195153
 - Sparsity_Different parameters: 1455
 - Different parameters: 196608
Layer base_model.model.adapters_itot.6.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 18487
 - Sparsity_Different parameters: 178121
 - Different parameters: 196608
Layer base_model.model.adapters_itot.7.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195178
 - Sparsity_Different parameters: 1430
 - Different parameters: 196608
Layer base_model.model.adapters_itot.7.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.7.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195084
 - Sparsity_Different parameters: 1524
 - Different parameters: 196608
Layer base_model.model.adapters_itot.7.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.7.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 194971
 - Sparsity_Different parameters: 1637
 - Different parameters: 196607
Layer base_model.model.adapters_itot.7.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 3880
 - Sparsity_Different parameters: 192728
 - Different parameters: 196608
Layer base_model.model.adapters_itot.7.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195008
 - Sparsity_Different parameters: 1600
 - Different parameters: 196608
Layer base_model.model.adapters_itot.7.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 19543
 - Sparsity_Different parameters: 177065
 - Different parameters: 196608
Layer base_model.model.adapters_itot.8.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195080
 - Sparsity_Different parameters: 1528
 - Different parameters: 196608
Layer base_model.model.adapters_itot.8.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.8.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195051
 - Sparsity_Different parameters: 1557
 - Different parameters: 196608
Layer base_model.model.adapters_itot.8.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.8.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195079
 - Sparsity_Different parameters: 1529
 - Different parameters: 196608
Layer base_model.model.adapters_itot.8.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 1903
 - Sparsity_Different parameters: 194705
 - Different parameters: 196608
Layer base_model.model.adapters_itot.8.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195063
 - Sparsity_Different parameters: 1545
 - Different parameters: 196608
Layer base_model.model.adapters_itot.8.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 17270
 - Sparsity_Different parameters: 179338
 - Different parameters: 196608
Layer base_model.model.adapters_itot.9.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195048
 - Sparsity_Different parameters: 1560
 - Different parameters: 196608
Layer base_model.model.adapters_itot.9.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.9.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195049
 - Sparsity_Different parameters: 1559
 - Different parameters: 196608
Layer base_model.model.adapters_itot.9.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.9.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195117
 - Sparsity_Different parameters: 1491
 - Different parameters: 196608
Layer base_model.model.adapters_itot.9.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 3563
 - Sparsity_Different parameters: 193045
 - Different parameters: 196608
Layer base_model.model.adapters_itot.9.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195117
 - Sparsity_Different parameters: 1491
 - Different parameters: 196608
Layer base_model.model.adapters_itot.9.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 19833
 - Sparsity_Different parameters: 176775
 - Different parameters: 196608
Layer base_model.model.adapters_itot.10.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195106
 - Sparsity_Different parameters: 1502
 - Different parameters: 196608
Layer base_model.model.adapters_itot.10.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.10.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195112
 - Sparsity_Different parameters: 1496
 - Different parameters: 196608
Layer base_model.model.adapters_itot.10.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.10.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195091
 - Sparsity_Different parameters: 1517
 - Different parameters: 196608
Layer base_model.model.adapters_itot.10.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 11040
 - Sparsity_Different parameters: 185568
 - Different parameters: 196608
Layer base_model.model.adapters_itot.10.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195086
 - Sparsity_Different parameters: 1522
 - Different parameters: 196608
Layer base_model.model.adapters_itot.10.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 21918
 - Sparsity_Different parameters: 174690
 - Different parameters: 196608
Layer base_model.model.adapters_itot.11.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195055
 - Sparsity_Different parameters: 1553
 - Different parameters: 196608
Layer base_model.model.adapters_itot.11.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.11.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195095
 - Sparsity_Different parameters: 1513
 - Different parameters: 196608
Layer base_model.model.adapters_itot.11.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.11.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195042
 - Sparsity_Different parameters: 1566
 - Different parameters: 196608
Layer base_model.model.adapters_itot.11.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 16146
 - Sparsity_Different parameters: 180462
 - Different parameters: 196608
Layer base_model.model.adapters_itot.11.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195042
 - Sparsity_Different parameters: 1566
 - Different parameters: 196608
Layer base_model.model.adapters_itot.11.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 21544
 - Sparsity_Different parameters: 175064
 - Different parameters: 196608
Layer base_model.model.adapters_itot.12.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195166
 - Sparsity_Different parameters: 1442
 - Different parameters: 196608
Layer base_model.model.adapters_itot.12.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.12.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195128
 - Sparsity_Different parameters: 1480
 - Different parameters: 196608
Layer base_model.model.adapters_itot.12.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.12.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195084
 - Sparsity_Different parameters: 1524
 - Different parameters: 196608
Layer base_model.model.adapters_itot.12.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 83677
 - Sparsity_Different parameters: 112931
 - Different parameters: 196608
Layer base_model.model.adapters_itot.12.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195129
 - Sparsity_Different parameters: 1479
 - Different parameters: 196608
Layer base_model.model.adapters_itot.12.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 31929
 - Sparsity_Different parameters: 164679
 - Different parameters: 196608
Layer base_model.model.adapters_itot.13.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195164
 - Sparsity_Different parameters: 1444
 - Different parameters: 196608
Layer base_model.model.adapters_itot.13.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.13.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195076
 - Sparsity_Different parameters: 1532
 - Different parameters: 196608
Layer base_model.model.adapters_itot.13.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.13.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195065
 - Sparsity_Different parameters: 1543
 - Different parameters: 196608
Layer base_model.model.adapters_itot.13.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 19622
 - Sparsity_Different parameters: 176986
 - Different parameters: 196608
Layer base_model.model.adapters_itot.13.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195040
 - Sparsity_Different parameters: 1568
 - Different parameters: 196608
Layer base_model.model.adapters_itot.13.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 19517
 - Sparsity_Different parameters: 177091
 - Different parameters: 196608
Layer base_model.model.adapters_itot.14.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195165
 - Sparsity_Different parameters: 1443
 - Different parameters: 196608
Layer base_model.model.adapters_itot.14.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.14.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195128
 - Sparsity_Different parameters: 1480
 - Different parameters: 196608
Layer base_model.model.adapters_itot.14.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.14.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195085
 - Sparsity_Different parameters: 1523
 - Different parameters: 196608
Layer base_model.model.adapters_itot.14.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 77573
 - Sparsity_Different parameters: 119035
 - Different parameters: 196608
Layer base_model.model.adapters_itot.14.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195042
 - Sparsity_Different parameters: 1566
 - Different parameters: 196608
Layer base_model.model.adapters_itot.14.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 32688
 - Sparsity_Different parameters: 163920
 - Different parameters: 196608
Layer base_model.model.adapters_itot.15.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195113
 - Sparsity_Different parameters: 1495
 - Different parameters: 196608
Layer base_model.model.adapters_itot.15.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.15.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195049
 - Sparsity_Different parameters: 1559
 - Different parameters: 196608
Layer base_model.model.adapters_itot.15.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.15.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195043
 - Sparsity_Different parameters: 1565
 - Different parameters: 196608
Layer base_model.model.adapters_itot.15.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 110123
 - Sparsity_Different parameters: 86485
 - Different parameters: 196608
Layer base_model.model.adapters_itot.15.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195068
 - Sparsity_Different parameters: 1540
 - Different parameters: 196608
Layer base_model.model.adapters_itot.15.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 48977
 - Sparsity_Different parameters: 147631
 - Different parameters: 196608
Layer base_model.model.adapters_itot.16.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195081
 - Sparsity_Different parameters: 1527
 - Different parameters: 196608
Layer base_model.model.adapters_itot.16.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.16.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195037
 - Sparsity_Different parameters: 1571
 - Different parameters: 196608
Layer base_model.model.adapters_itot.16.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.16.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195068
 - Sparsity_Different parameters: 1540
 - Different parameters: 196608
Layer base_model.model.adapters_itot.16.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 41391
 - Sparsity_Different parameters: 155217
 - Different parameters: 196608
Layer base_model.model.adapters_itot.16.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195055
 - Sparsity_Different parameters: 1553
 - Different parameters: 196608
Layer base_model.model.adapters_itot.16.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 25749
 - Sparsity_Different parameters: 170859
 - Different parameters: 196608
Layer base_model.model.adapters_itot.17.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195087
 - Sparsity_Different parameters: 1521
 - Different parameters: 196608
Layer base_model.model.adapters_itot.17.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.17.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195044
 - Sparsity_Different parameters: 1564
 - Different parameters: 196608
Layer base_model.model.adapters_itot.17.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.17.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195100
 - Sparsity_Different parameters: 1508
 - Different parameters: 196608
Layer base_model.model.adapters_itot.17.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 47119
 - Sparsity_Different parameters: 149489
 - Different parameters: 196608
Layer base_model.model.adapters_itot.17.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195011
 - Sparsity_Different parameters: 1597
 - Different parameters: 196608
Layer base_model.model.adapters_itot.17.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 28769
 - Sparsity_Different parameters: 167839
 - Different parameters: 196608
Layer base_model.model.adapters_itot.18.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195064
 - Sparsity_Different parameters: 1544
 - Different parameters: 196608
Layer base_model.model.adapters_itot.18.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.18.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195085
 - Sparsity_Different parameters: 1523
 - Different parameters: 196608
Layer base_model.model.adapters_itot.18.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.18.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195133
 - Sparsity_Different parameters: 1475
 - Different parameters: 196608
Layer base_model.model.adapters_itot.18.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 48304
 - Sparsity_Different parameters: 148304
 - Different parameters: 196608
Layer base_model.model.adapters_itot.18.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195116
 - Sparsity_Different parameters: 1492
 - Different parameters: 196608
Layer base_model.model.adapters_itot.18.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 24364
 - Sparsity_Different parameters: 172244
 - Different parameters: 196608
Layer base_model.model.adapters_itot.19.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 194964
 - Sparsity_Different parameters: 1644
 - Different parameters: 196608
Layer base_model.model.adapters_itot.19.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.19.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195123
 - Sparsity_Different parameters: 1485
 - Different parameters: 196608
Layer base_model.model.adapters_itot.19.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.19.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195113
 - Sparsity_Different parameters: 1495
 - Different parameters: 196608
Layer base_model.model.adapters_itot.19.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 24323
 - Sparsity_Different parameters: 172285
 - Different parameters: 196608
Layer base_model.model.adapters_itot.19.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195093
 - Sparsity_Different parameters: 1515
 - Different parameters: 196608
Layer base_model.model.adapters_itot.19.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 18992
 - Sparsity_Different parameters: 177616
 - Different parameters: 196608
Layer base_model.model.adapters_itot.20.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195085
 - Sparsity_Different parameters: 1523
 - Different parameters: 196608
Layer base_model.model.adapters_itot.20.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.20.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195135
 - Sparsity_Different parameters: 1473
 - Different parameters: 196608
Layer base_model.model.adapters_itot.20.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.20.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195017
 - Sparsity_Different parameters: 1591
 - Different parameters: 196608
Layer base_model.model.adapters_itot.20.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 21634
 - Sparsity_Different parameters: 174974
 - Different parameters: 196608
Layer base_model.model.adapters_itot.20.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195061
 - Sparsity_Different parameters: 1547
 - Different parameters: 196608
Layer base_model.model.adapters_itot.20.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 22029
 - Sparsity_Different parameters: 174579
 - Different parameters: 196608
Layer base_model.model.adapters_itot.21.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195029
 - Sparsity_Different parameters: 1579
 - Different parameters: 196608
Layer base_model.model.adapters_itot.21.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.21.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195007
 - Sparsity_Different parameters: 1601
 - Different parameters: 196608
Layer base_model.model.adapters_itot.21.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.21.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195075
 - Sparsity_Different parameters: 1533
 - Different parameters: 196608
Layer base_model.model.adapters_itot.21.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 7473
 - Sparsity_Different parameters: 189135
 - Different parameters: 196608
Layer base_model.model.adapters_itot.21.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195080
 - Sparsity_Different parameters: 1528
 - Different parameters: 196608
Layer base_model.model.adapters_itot.21.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 19018
 - Sparsity_Different parameters: 177590
 - Different parameters: 196608
Layer base_model.model.adapters_itot.22.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195089
 - Sparsity_Different parameters: 1519
 - Different parameters: 196608
Layer base_model.model.adapters_itot.22.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.22.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195122
 - Sparsity_Different parameters: 1486
 - Different parameters: 196608
Layer base_model.model.adapters_itot.22.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.22.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195035
 - Sparsity_Different parameters: 1573
 - Different parameters: 196608
Layer base_model.model.adapters_itot.22.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 3092
 - Sparsity_Different parameters: 193516
 - Different parameters: 196608
Layer base_model.model.adapters_itot.22.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195089
 - Sparsity_Different parameters: 1519
 - Different parameters: 196608
Layer base_model.model.adapters_itot.22.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 18026
 - Sparsity_Different parameters: 178582
 - Different parameters: 196608
Layer base_model.model.adapters_itot.23.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195089
 - Sparsity_Different parameters: 1519
 - Different parameters: 196608
Layer base_model.model.adapters_itot.23.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.23.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195001
 - Sparsity_Different parameters: 1607
 - Different parameters: 196608
Layer base_model.model.adapters_itot.23.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 193149
 - Sparsity_Different parameters: 3459
 - Different parameters: 196608
Layer base_model.model.adapters_itot.23.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195079
 - Sparsity_Different parameters: 1529
 - Different parameters: 196608
Layer base_model.model.adapters_itot.23.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 2836
 - Sparsity_Different parameters: 193772
 - Different parameters: 196608
Layer base_model.model.adapters_itot.23.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195087
 - Sparsity_Different parameters: 1521
 - Different parameters: 196608
Layer base_model.model.adapters_itot.23.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 19241
 - Sparsity_Different parameters: 177367
 - Different parameters: 196608
Layer base_model.model.adapters_itot.24.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195107
 - Sparsity_Different parameters: 1501
 - Different parameters: 196608
Layer base_model.model.adapters_itot.24.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.24.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195078
 - Sparsity_Different parameters: 1530
 - Different parameters: 196608
Layer base_model.model.adapters_itot.24.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 162382
 - Sparsity_Different parameters: 34226
 - Different parameters: 196608
Layer base_model.model.adapters_itot.24.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195072
 - Sparsity_Different parameters: 1536
 - Different parameters: 196608
Layer base_model.model.adapters_itot.24.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 1501
 - Sparsity_Different parameters: 195107
 - Different parameters: 196608
Layer base_model.model.adapters_itot.24.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195062
 - Sparsity_Different parameters: 1546
 - Different parameters: 196608
Layer base_model.model.adapters_itot.24.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 14897
 - Sparsity_Different parameters: 181711
 - Different parameters: 196608
Layer base_model.model.adapters_itot.25.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195025
 - Sparsity_Different parameters: 1583
 - Different parameters: 196608
Layer base_model.model.adapters_itot.25.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.25.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195053
 - Sparsity_Different parameters: 1555
 - Different parameters: 196608
Layer base_model.model.adapters_itot.25.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.25.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195090
 - Sparsity_Different parameters: 1518
 - Different parameters: 196608
Layer base_model.model.adapters_itot.25.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 4534
 - Sparsity_Different parameters: 192074
 - Different parameters: 196608
Layer base_model.model.adapters_itot.25.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 194998
 - Sparsity_Different parameters: 1610
 - Different parameters: 196608
Layer base_model.model.adapters_itot.25.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 18315
 - Sparsity_Different parameters: 178293
 - Different parameters: 196608
Layer base_model.model.adapters_itot.26.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195029
 - Sparsity_Different parameters: 1579
 - Different parameters: 196608
Layer base_model.model.adapters_itot.26.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.26.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195077
 - Sparsity_Different parameters: 1531
 - Different parameters: 196608
Layer base_model.model.adapters_itot.26.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.26.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195058
 - Sparsity_Different parameters: 1550
 - Different parameters: 196608
Layer base_model.model.adapters_itot.26.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 7439
 - Sparsity_Different parameters: 189169
 - Different parameters: 196608
Layer base_model.model.adapters_itot.26.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195041
 - Sparsity_Different parameters: 1567
 - Different parameters: 196608
Layer base_model.model.adapters_itot.26.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 22672
 - Sparsity_Different parameters: 173936
 - Different parameters: 196608
Layer base_model.model.adapters_itot.27.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195053
 - Sparsity_Different parameters: 1555
 - Different parameters: 196608
Layer base_model.model.adapters_itot.27.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.27.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195037
 - Sparsity_Different parameters: 1571
 - Different parameters: 196608
Layer base_model.model.adapters_itot.27.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.27.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195070
 - Sparsity_Different parameters: 1538
 - Different parameters: 196608
Layer base_model.model.adapters_itot.27.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 35148
 - Sparsity_Different parameters: 161460
 - Different parameters: 196608
Layer base_model.model.adapters_itot.27.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195047
 - Sparsity_Different parameters: 1561
 - Different parameters: 196608
Layer base_model.model.adapters_itot.27.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 34846
 - Sparsity_Different parameters: 161762
 - Different parameters: 196608
Layer base_model.model.adapters_itot.28.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195102
 - Sparsity_Different parameters: 1506
 - Different parameters: 196608
Layer base_model.model.adapters_itot.28.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.28.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195120
 - Sparsity_Different parameters: 1488
 - Different parameters: 196608
Layer base_model.model.adapters_itot.28.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_itot.28.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195081
 - Sparsity_Different parameters: 1527
 - Different parameters: 196608
Layer base_model.model.adapters_itot.28.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 76084
 - Sparsity_Different parameters: 120524
 - Different parameters: 196608
Layer base_model.model.adapters_itot.28.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195085
 - Sparsity_Different parameters: 1523
 - Different parameters: 196608
Layer base_model.model.adapters_itot.28.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 52650
 - Sparsity_Different parameters: 143958
 - Different parameters: 196608
Layer base_model.model.adapters_itot.29.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195081
 - Sparsity_Different parameters: 1527
 - Different parameters: 196608
Layer base_model.model.adapters_itot.29.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 0
Layer base_model.model.adapters_itot.29.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195066
 - Sparsity_Different parameters: 1542
 - Different parameters: 196608
Layer base_model.model.adapters_itot.29.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 0
Layer base_model.model.adapters_itot.29.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195052
 - Sparsity_Different parameters: 1556
 - Different parameters: 196608
Layer base_model.model.adapters_itot.29.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 0
Layer base_model.model.adapters_itot.29.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195098
 - Sparsity_Different parameters: 1510
 - Different parameters: 196608
Layer base_model.model.adapters_itot.29.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 0
Layer base_model.model.adapters_ttoi.0.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195084
 - Sparsity_Different parameters: 1524
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.0.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.0.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195004
 - Sparsity_Different parameters: 1604
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.0.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.0.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195085
 - Sparsity_Different parameters: 1523
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.0.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 79572
 - Sparsity_Different parameters: 117036
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.0.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195073
 - Sparsity_Different parameters: 1535
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.0.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 39570
 - Sparsity_Different parameters: 157038
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.1.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195092
 - Sparsity_Different parameters: 1516
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.1.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.1.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195063
 - Sparsity_Different parameters: 1545
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.1.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.1.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195150
 - Sparsity_Different parameters: 1458
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.1.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 19671
 - Sparsity_Different parameters: 176937
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.1.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195038
 - Sparsity_Different parameters: 1570
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.1.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 35117
 - Sparsity_Different parameters: 161491
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.2.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195096
 - Sparsity_Different parameters: 1512
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.2.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.2.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195072
 - Sparsity_Different parameters: 1536
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.2.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.2.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195109
 - Sparsity_Different parameters: 1499
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.2.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 19554
 - Sparsity_Different parameters: 177054
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.2.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195064
 - Sparsity_Different parameters: 1544
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.2.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 35037
 - Sparsity_Different parameters: 161571
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.3.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195006
 - Sparsity_Different parameters: 1602
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.3.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.3.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195016
 - Sparsity_Different parameters: 1592
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.3.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.3.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195045
 - Sparsity_Different parameters: 1563
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.3.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 26835
 - Sparsity_Different parameters: 169773
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.3.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195091
 - Sparsity_Different parameters: 1517
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.3.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 34065
 - Sparsity_Different parameters: 162543
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.4.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195096
 - Sparsity_Different parameters: 1512
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.4.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.4.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195080
 - Sparsity_Different parameters: 1528
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.4.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.4.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195092
 - Sparsity_Different parameters: 1516
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.4.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 27937
 - Sparsity_Different parameters: 168671
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.4.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195052
 - Sparsity_Different parameters: 1556
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.4.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 33657
 - Sparsity_Different parameters: 162951
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.5.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195071
 - Sparsity_Different parameters: 1537
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.5.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.5.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195136
 - Sparsity_Different parameters: 1472
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.5.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.5.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195099
 - Sparsity_Different parameters: 1509
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.5.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 17597
 - Sparsity_Different parameters: 179011
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.5.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195123
 - Sparsity_Different parameters: 1485
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.5.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 31116
 - Sparsity_Different parameters: 165492
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.6.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195082
 - Sparsity_Different parameters: 1526
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.6.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.6.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195087
 - Sparsity_Different parameters: 1521
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.6.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.6.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195100
 - Sparsity_Different parameters: 1508
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.6.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 43377
 - Sparsity_Different parameters: 153231
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.6.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195033
 - Sparsity_Different parameters: 1575
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.6.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 36630
 - Sparsity_Different parameters: 159978
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.7.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195111
 - Sparsity_Different parameters: 1497
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.7.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.7.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195071
 - Sparsity_Different parameters: 1537
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.7.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.7.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195076
 - Sparsity_Different parameters: 1532
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.7.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 17744
 - Sparsity_Different parameters: 178864
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.7.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195089
 - Sparsity_Different parameters: 1519
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.7.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 28419
 - Sparsity_Different parameters: 168189
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.8.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195075
 - Sparsity_Different parameters: 1533
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.8.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.8.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195086
 - Sparsity_Different parameters: 1522
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.8.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.8.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195015
 - Sparsity_Different parameters: 1593
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.8.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 12156
 - Sparsity_Different parameters: 184452
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.8.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195063
 - Sparsity_Different parameters: 1545
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.8.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 30155
 - Sparsity_Different parameters: 166453
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.9.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195096
 - Sparsity_Different parameters: 1512
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.9.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.9.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195142
 - Sparsity_Different parameters: 1466
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.9.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.9.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195053
 - Sparsity_Different parameters: 1555
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.9.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 9899
 - Sparsity_Different parameters: 186709
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.9.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195055
 - Sparsity_Different parameters: 1553
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.9.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 25944
 - Sparsity_Different parameters: 170664
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.10.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195052
 - Sparsity_Different parameters: 1556
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.10.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.10.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195090
 - Sparsity_Different parameters: 1518
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.10.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.10.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195106
 - Sparsity_Different parameters: 1502
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.10.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 1543
 - Sparsity_Different parameters: 195065
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.10.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195074
 - Sparsity_Different parameters: 1534
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.10.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 24180
 - Sparsity_Different parameters: 172428
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.11.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195059
 - Sparsity_Different parameters: 1549
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.11.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.11.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195003
 - Sparsity_Different parameters: 1605
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.11.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.11.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195068
 - Sparsity_Different parameters: 1540
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.11.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 1140
 - Sparsity_Different parameters: 195468
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.11.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195119
 - Sparsity_Different parameters: 1489
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.11.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 20454
 - Sparsity_Different parameters: 176154
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.12.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195051
 - Sparsity_Different parameters: 1557
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.12.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.12.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195064
 - Sparsity_Different parameters: 1544
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.12.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.12.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195054
 - Sparsity_Different parameters: 1554
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.12.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 363
 - Sparsity_Different parameters: 196245
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.12.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195111
 - Sparsity_Different parameters: 1497
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.12.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 18751
 - Sparsity_Different parameters: 177857
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.13.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195056
 - Sparsity_Different parameters: 1552
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.13.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.13.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195052
 - Sparsity_Different parameters: 1556
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.13.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.13.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195078
 - Sparsity_Different parameters: 1530
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.13.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 266
 - Sparsity_Different parameters: 196342
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.13.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195012
 - Sparsity_Different parameters: 1596
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.13.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 20421
 - Sparsity_Different parameters: 176187
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.14.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195037
 - Sparsity_Different parameters: 1571
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.14.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.14.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195152
 - Sparsity_Different parameters: 1456
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.14.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.14.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195111
 - Sparsity_Different parameters: 1497
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.14.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 732
 - Sparsity_Different parameters: 195876
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.14.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195134
 - Sparsity_Different parameters: 1474
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.14.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 18916
 - Sparsity_Different parameters: 177692
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.15.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195105
 - Sparsity_Different parameters: 1503
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.15.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.15.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195092
 - Sparsity_Different parameters: 1516
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.15.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.15.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195126
 - Sparsity_Different parameters: 1482
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.15.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 3774
 - Sparsity_Different parameters: 192834
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.15.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195074
 - Sparsity_Different parameters: 1534
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.15.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 21860
 - Sparsity_Different parameters: 174748
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.16.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195118
 - Sparsity_Different parameters: 1490
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.16.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.16.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195066
 - Sparsity_Different parameters: 1542
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.16.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.16.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195077
 - Sparsity_Different parameters: 1531
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.16.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 128
 - Sparsity_Different parameters: 196480
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.16.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195020
 - Sparsity_Different parameters: 1588
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.16.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 18139
 - Sparsity_Different parameters: 178469
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.17.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195077
 - Sparsity_Different parameters: 1531
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.17.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.17.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195038
 - Sparsity_Different parameters: 1570
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.17.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.17.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195183
 - Sparsity_Different parameters: 1425
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.17.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 86
 - Sparsity_Different parameters: 196522
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.17.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195121
 - Sparsity_Different parameters: 1487
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.17.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 16414
 - Sparsity_Different parameters: 180194
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.18.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195119
 - Sparsity_Different parameters: 1489
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.18.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.18.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195077
 - Sparsity_Different parameters: 1531
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.18.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.18.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195100
 - Sparsity_Different parameters: 1508
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.18.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 1726
 - Sparsity_Different parameters: 194882
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.18.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195006
 - Sparsity_Different parameters: 1602
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.18.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 22344
 - Sparsity_Different parameters: 174264
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.19.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195088
 - Sparsity_Different parameters: 1520
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.19.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.19.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195009
 - Sparsity_Different parameters: 1599
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.19.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.19.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195053
 - Sparsity_Different parameters: 1555
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.19.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 1441
 - Sparsity_Different parameters: 195167
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.19.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195143
 - Sparsity_Different parameters: 1465
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.19.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 22333
 - Sparsity_Different parameters: 174275
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.20.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195080
 - Sparsity_Different parameters: 1528
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.20.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.20.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195017
 - Sparsity_Different parameters: 1591
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.20.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.20.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195037
 - Sparsity_Different parameters: 1571
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.20.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 5581
 - Sparsity_Different parameters: 191027
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.20.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195081
 - Sparsity_Different parameters: 1527
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.20.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 29369
 - Sparsity_Different parameters: 167239
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.21.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195136
 - Sparsity_Different parameters: 1472
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.21.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.21.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195064
 - Sparsity_Different parameters: 1544
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.21.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.21.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195077
 - Sparsity_Different parameters: 1531
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.21.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 23491
 - Sparsity_Different parameters: 173117
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.21.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195081
 - Sparsity_Different parameters: 1527
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.21.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 35015
 - Sparsity_Different parameters: 161593
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.22.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195046
 - Sparsity_Different parameters: 1562
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.22.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.22.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195033
 - Sparsity_Different parameters: 1575
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.22.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.22.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195120
 - Sparsity_Different parameters: 1488
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.22.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 31400
 - Sparsity_Different parameters: 165208
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.22.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195099
 - Sparsity_Different parameters: 1509
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.22.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 44913
 - Sparsity_Different parameters: 151695
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.23.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195040
 - Sparsity_Different parameters: 1568
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.23.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.23.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195073
 - Sparsity_Different parameters: 1535
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.23.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.23.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195124
 - Sparsity_Different parameters: 1484
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.23.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 63950
 - Sparsity_Different parameters: 132658
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.23.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195059
 - Sparsity_Different parameters: 1549
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.23.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 48758
 - Sparsity_Different parameters: 147850
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.24.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195064
 - Sparsity_Different parameters: 1544
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.24.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.24.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195033
 - Sparsity_Different parameters: 1575
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.24.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.24.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195094
 - Sparsity_Different parameters: 1514
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.24.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 53950
 - Sparsity_Different parameters: 142658
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.24.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195106
 - Sparsity_Different parameters: 1502
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.24.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 51072
 - Sparsity_Different parameters: 145536
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.25.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195091
 - Sparsity_Different parameters: 1517
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.25.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.25.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195076
 - Sparsity_Different parameters: 1532
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.25.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.25.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195092
 - Sparsity_Different parameters: 1516
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.25.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 31186
 - Sparsity_Different parameters: 165422
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.25.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195076
 - Sparsity_Different parameters: 1532
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.25.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 45008
 - Sparsity_Different parameters: 151600
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.26.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195066
 - Sparsity_Different parameters: 1542
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.26.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.26.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195121
 - Sparsity_Different parameters: 1487
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.26.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.26.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195056
 - Sparsity_Different parameters: 1552
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.26.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 7669
 - Sparsity_Different parameters: 188939
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.26.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195096
 - Sparsity_Different parameters: 1512
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.26.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 23992
 - Sparsity_Different parameters: 172616
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.27.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195053
 - Sparsity_Different parameters: 1555
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.27.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.27.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195107
 - Sparsity_Different parameters: 1501
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.27.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.27.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195098
 - Sparsity_Different parameters: 1510
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.27.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 23478
 - Sparsity_Different parameters: 173130
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.27.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195087
 - Sparsity_Different parameters: 1521
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.27.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 42119
 - Sparsity_Different parameters: 154489
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.28.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195096
 - Sparsity_Different parameters: 1512
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.28.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.28.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195019
 - Sparsity_Different parameters: 1589
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.28.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.28.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195099
 - Sparsity_Different parameters: 1509
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.28.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 44855
 - Sparsity_Different parameters: 151753
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.28.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195113
 - Sparsity_Different parameters: 1495
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.28.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 56877
 - Sparsity_Different parameters: 139731
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.29.to_q.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195121
 - Sparsity_Different parameters: 1487
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.29.to_q.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.29.to_k.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195030
 - Sparsity_Different parameters: 1578
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.29.to_k.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 196608
 - Sparsity_Different parameters: 0
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.29.to_v.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195095
 - Sparsity_Different parameters: 1513
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.29.to_v.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 52013
 - Sparsity_Different parameters: 144595
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.29.to_out.lora_A.default.weight (torch.Size([128, 1536])):
 - Same sparsity positions: 195097
 - Sparsity_Different parameters: 1511
 - Different parameters: 196608
Layer base_model.model.adapters_ttoi.29.to_out.lora_B.default.weight (torch.Size([1536, 128])):
 - Same sparsity positions: 32731
 - Sparsity_Different parameters: 163877
 - Different parameters: 196608
sparsity 68019637
total tensor(191768573)
